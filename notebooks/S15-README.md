# Session 15 - Data Cleaning & Feature Engineering

## üéØ Objectifs de la session
- Ma√Ætriser les techniques de nettoyage de donn√©es
- G√©rer les valeurs manquantes avec diff√©rentes strat√©gies
- Identifier et traiter les outliers
- Cr√©er de nouvelles variables pertinentes (feature engineering)
- Normaliser et transformer les donn√©es

---

## üìö Partie 1 : Comprendre les donn√©es sales

### Pourquoi nettoyer les donn√©es ?
**"Garbage in, garbage out"** - La qualit√© de vos analyses d√©pend de la qualit√© de vos donn√©es.

Les donn√©es r√©elles contiennent souvent :
- **Valeurs manquantes** : cellules vides, NaN, NULL
- **Doublons** : m√™mes observations r√©p√©t√©es
- **Outliers** : valeurs extr√™mes ou aberrantes
- **Incoh√©rences** : erreurs de saisie, formats diff√©rents
- **Types incorrects** : nombres stock√©s comme texte, etc.

### Impact des donn√©es sales
- **Biais dans les analyses** : r√©sultats fauss√©s
- **Mod√®les peu performants** : pr√©dictions incorrectes
- **Erreurs de calcul** : statistiques invalides
- **Perte de temps** : debugging et corrections

---

## üîç Partie 2 : Diagnostic des donn√©es

### Exploration initiale
```python
import pandas as pd
import numpy as np

# Charger les donn√©es
df = pd.read_csv('data/titanic.csv')

# Vue d'ensemble
print(df.shape)              # Dimensions
print(df.head())             # Premi√®res lignes
print(df.info())             # Types et valeurs non-null
print(df.describe())         # Statistiques descriptives
```

### D√©tection des valeurs manquantes
```python
# Compter les valeurs manquantes
missing_count = df.isnull().sum()
print(missing_count[missing_count > 0])

# Pourcentage de valeurs manquantes
missing_pct = (df.isnull().sum() / len(df) * 100).round(2)
print(missing_pct[missing_pct > 0])

# Visualiser les patterns de valeurs manquantes
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
plt.title('Patterns de valeurs manquantes')
plt.show()
```

### D√©tection des doublons
```python
# V√©rifier les doublons
duplicates = df.duplicated()
print(f"Nombre de doublons : {duplicates.sum()}")

# Voir les doublons
print(df[df.duplicated(keep=False)])  # keep=False pour voir tous les doublons

# Doublons sur certaines colonnes seulement
duplicates_subset = df.duplicated(subset=['Name', 'Age'])
print(f"Doublons sur Name et Age : {duplicates_subset.sum()}")
```

### D√©tection des outliers
```python
# M√©thode 1 : R√®gle des 1.5*IQR (InterQuartile Range)
Q1 = df['Fare'].quantile(0.25)
Q3 = df['Fare'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = df[(df['Fare'] < lower_bound) | (df['Fare'] > upper_bound)]
print(f"Outliers d√©tect√©s : {len(outliers)}")

# M√©thode 2 : Z-score (pour distribution normale)
from scipy import stats
z_scores = np.abs(stats.zscore(df['Fare'].dropna()))
outliers_zscore = df[z_scores > 3]  # Au-del√† de 3 √©cart-types
print(f"Outliers (Z-score > 3) : {len(outliers_zscore)}")

# Visualisation des outliers
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.boxplot(df['Fare'].dropna())
plt.title('Boxplot - Fare')
plt.subplot(1, 2, 2)
plt.hist(df['Fare'].dropna(), bins=50)
plt.title('Distribution - Fare')
plt.show()
```

---

## üßπ Partie 3 : Traitement des valeurs manquantes

### Strat√©gie 1 : Suppression

**Supprimer les lignes** (quand peu de valeurs manquantes)
```python
# Supprimer les lignes avec au moins une valeur manquante
df_dropped = df.dropna()
print(f"Lignes restantes : {len(df_dropped)} / {len(df)}")

# Supprimer les lignes avec valeurs manquantes sur certaines colonnes
df_dropped_subset = df.dropna(subset=['Age', 'Embarked'])

# Supprimer si toutes les valeurs sont manquantes
df_dropped_all = df.dropna(how='all')
```

**Supprimer les colonnes** (si trop de valeurs manquantes)
```python
# Supprimer les colonnes avec > 50% de valeurs manquantes
threshold = 0.5
missing_pct = df.isnull().sum() / len(df)
cols_to_drop = missing_pct[missing_pct > threshold].index
df_cleaned = df.drop(columns=cols_to_drop)
```

### Strat√©gie 2 : Imputation simple

**Imputation par la moyenne/m√©diane** (variables num√©riques)
```python
# Moyenne
df['Age'].fillna(df['Age'].mean(), inplace=True)

# M√©diane (plus robuste aux outliers)
df['Age'].fillna(df['Age'].median(), inplace=True)

# Mode (valeur la plus fr√©quente)
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)
```

**Imputation par une valeur constante**
```python
# Remplacer par 0
df['Cabin'].fillna('Unknown', inplace=True)

# Remplacer par une valeur sp√©cifique
df['Age'].fillna(-1, inplace=True)  # -1 pour "non renseign√©"
```

**Forward fill / Backward fill** (donn√©es temporelles)
```python
# Forward fill : propager la derni√®re valeur valide
df['Price'].fillna(method='ffill', inplace=True)

# Backward fill : propager la prochaine valeur valide
df['Price'].fillna(method='bfill', inplace=True)
```

### Strat√©gie 3 : Imputation avanc√©e

**Imputation conditionnelle**
```python
# Exemple : √Çge moyen selon la classe et le sexe
age_by_group = df.groupby(['Pclass', 'Sex'])['Age'].transform('median')
df['Age'] = df['Age'].fillna(age_by_group)
```

**Imputation par r√©gression** (sklearn)
```python
from sklearn.impute import SimpleImputer, KNNImputer

# SimpleImputer (moyenne, m√©diane, mode)
imputer = SimpleImputer(strategy='median')
df[['Age']] = imputer.fit_transform(df[['Age']])

# KNNImputer (k plus proches voisins)
imputer = KNNImputer(n_neighbors=5)
df[['Age', 'Fare']] = imputer.fit_transform(df[['Age', 'Fare']])
```

### Strat√©gie 4 : Cr√©er un indicateur de manque
```python
# Cr√©er une colonne binaire indiquant si la valeur √©tait manquante
df['Age_was_missing'] = df['Age'].isnull().astype(int)

# Puis imputer
df['Age'].fillna(df['Age'].median(), inplace=True)
```

---

## üéØ Partie 4 : Traitement des outliers

### M√©thode 1 : Suppression
```python
# Supprimer les outliers d√©tect√©s par IQR
Q1 = df['Fare'].quantile(0.25)
Q3 = df['Fare'].quantile(0.75)
IQR = Q3 - Q1
df_no_outliers = df[
    (df['Fare'] >= Q1 - 1.5 * IQR) & 
    (df['Fare'] <= Q3 + 1.5 * IQR)
]
```

### M√©thode 2 : Winsorization (capping)
```python
# Limiter les valeurs extr√™mes
lower_percentile = df['Fare'].quantile(0.01)
upper_percentile = df['Fare'].quantile(0.99)

df['Fare_capped'] = df['Fare'].clip(lower=lower_percentile, 
                                     upper=upper_percentile)
```

### M√©thode 3 : Transformation
```python
# Transformation logarithmique (r√©duire l'impact des outliers)
df['Fare_log'] = np.log1p(df['Fare'])  # log1p = log(1 + x)

# Transformation racine carr√©e
df['Fare_sqrt'] = np.sqrt(df['Fare'])
```

### M√©thode 4 : Garder mais flaguer
```python
# Cr√©er un indicateur d'outlier
Q1 = df['Fare'].quantile(0.25)
Q3 = df['Fare'].quantile(0.75)
IQR = Q3 - Q1
df['is_outlier'] = (
    (df['Fare'] < Q1 - 1.5 * IQR) | 
    (df['Fare'] > Q3 + 1.5 * IQR)
).astype(int)
```

---

## üîÑ Partie 5 : Transformation et normalisation

### Conversion de types
```python
# Convertir en num√©rique
df['Age'] = pd.to_numeric(df['Age'], errors='coerce')  # NaN si erreur

# Convertir en cat√©gorique
df['Pclass'] = df['Pclass'].astype('category')

# Convertir en datetime
df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d', errors='coerce')

# Convertir True/False en 1/0
df['Survived'] = df['Survived'].astype(int)
```

### Encodage des variables cat√©gorielles

**Label Encoding** (ordinales : ordre important)
```python
# Exemple : Pclass (1, 2, 3 ont un ordre)
df['Pclass_encoded'] = df['Pclass'].map({1: 1, 2: 2, 3: 3})

# Ou avec LabelEncoder
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['Sex_encoded'] = le.fit_transform(df['Sex'])  # male=1, female=0
```

**One-Hot Encoding** (nominales : pas d'ordre)
```python
# M√©thode pandas
df_encoded = pd.get_dummies(df, columns=['Embarked'], prefix='Embarked')
# Cr√©e : Embarked_C, Embarked_Q, Embarked_S

# M√©thode sklearn
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(sparse=False, drop='first')  # drop pour √©viter multicolin√©arit√©
encoded = encoder.fit_transform(df[['Embarked']])
```

### Normalisation et standardisation

**Min-Max Scaling** (normalisation entre 0 et 1)
```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df['Fare_normalized'] = scaler.fit_transform(df[['Fare']])
# Formule : (x - min) / (max - min)
```

**Standardisation** (Z-score : moyenne=0, √©cart-type=1)
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df['Fare_standardized'] = scaler.fit_transform(df[['Fare']])
# Formule : (x - mean) / std
```

**Robust Scaling** (utilise m√©diane et IQR, robuste aux outliers)
```python
from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
df['Fare_robust'] = scaler.fit_transform(df[['Fare']])
```

---

## ‚öôÔ∏è Partie 6 : Feature Engineering

### Cr√©ation de nouvelles variables

**√Ä partir de variables existantes**
```python
# Taille de la famille
df['FamilySize'] = df['SibSp'] + df['Parch'] + 1

# Indicateur de voyage seul
df['IsAlone'] = (df['FamilySize'] == 1).astype(int)

# Cat√©gories d'√¢ge
df['AgeGroup'] = pd.cut(df['Age'], 
                        bins=[0, 12, 18, 60, 100], 
                        labels=['Enfant', 'Adolescent', 'Adulte', 'Senior'])

# Cat√©gories de prix
df['FareRange'] = pd.qcut(df['Fare'], q=4, 
                          labels=['Bas', 'Moyen', '√âlev√©', 'Tr√®s √©lev√©'])
```

**Extraction d'informations**
```python
# Extraire le titre du nom
df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)

# Simplifier les titres rares
title_mapping = {
    'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',
    'Dr': 'Rare', 'Rev': 'Rare', 'Col': 'Rare', 'Major': 'Rare',
    'Mlle': 'Miss', 'Mme': 'Mrs', 'Ms': 'Miss', 'Lady': 'Rare',
    'Countess': 'Rare', 'Capt': 'Rare', 'Jonkheer': 'Rare', 'Don': 'Rare',
    'Dona': 'Rare', 'Sir': 'Rare'
}
df['Title'] = df['Title'].map(title_mapping)

# Indicateur de cabine connue
df['HasCabin'] = df['Cabin'].notna().astype(int)
```

**Interactions entre variables**
```python
# Cr√©er des interactions
df['Pclass_Sex'] = df['Pclass'].astype(str) + '_' + df['Sex']

# Produit de variables
df['Age_Fare_Interaction'] = df['Age'] * df['Fare']
```

**Variables bas√©es sur des conditions**
```python
# Femme ou enfant
df['WomanOrChild'] = ((df['Sex'] == 'female') | (df['Age'] < 18)).astype(int)

# Classe sup√©rieure
df['UpperClass'] = (df['Pclass'] <= 2).astype(int)
```

### Binning (discr√©tisation)
```python
# Binning avec intervalles √©gaux
df['Age_bins'] = pd.cut(df['Age'], bins=5)

# Binning avec quantiles (m√™me nombre d'√©l√©ments par bin)
df['Fare_quantiles'] = pd.qcut(df['Fare'], q=4)

# Binning personnalis√©
bins = [0, 18, 30, 50, 100]
labels = ['Jeune', 'Adulte', 'Mature', 'Senior']
df['AgeCategory'] = pd.cut(df['Age'], bins=bins, labels=labels)
```

---

## üìä Partie 7 : Workflow complet de nettoyage

```python
import pandas as pd
import numpy as np

def clean_titanic_data(df):
    """
    Nettoie le dataset Titanic et cr√©e de nouvelles features
    """
    # Copie pour ne pas modifier l'original
    df_clean = df.copy()
    
    # 1. Supprimer les colonnes inutiles
    df_clean = df_clean.drop(['PassengerId', 'Ticket', 'Cabin'], axis=1)
    
    # 2. Traiter les valeurs manquantes
    # Age : imputation par m√©diane selon classe et sexe
    df_clean['Age'] = df_clean.groupby(['Pclass', 'Sex'])['Age'].transform(
        lambda x: x.fillna(x.median())
    )
    
    # Embarked : mode (valeur la plus fr√©quente)
    df_clean['Embarked'].fillna(df_clean['Embarked'].mode()[0], inplace=True)
    
    # Fare : m√©diane
    df_clean['Fare'].fillna(df_clean['Fare'].median(), inplace=True)
    
    # 3. Cr√©er de nouvelles features
    df_clean['FamilySize'] = df_clean['SibSp'] + df_clean['Parch'] + 1
    df_clean['IsAlone'] = (df_clean['FamilySize'] == 1).astype(int)
    
    # Extraire le titre
    df_clean['Title'] = df_clean['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)
    title_mapping = {
        'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master'
    }
    df_clean['Title'] = df_clean['Title'].map(lambda x: title_mapping.get(x, 'Other'))
    
    # Cat√©gories d'√¢ge
    df_clean['AgeGroup'] = pd.cut(df_clean['Age'], 
                                   bins=[0, 12, 18, 60, 100], 
                                   labels=['Child', 'Teen', 'Adult', 'Senior'])
    
    # 4. Encodage
    df_clean['Sex'] = df_clean['Sex'].map({'male': 0, 'female': 1})
    df_clean = pd.get_dummies(df_clean, columns=['Embarked', 'Title'], drop_first=True)
    
    # 5. Supprimer la colonne Name (plus n√©cessaire)
    df_clean = df_clean.drop('Name', axis=1)
    
    return df_clean

# Utilisation
df_original = pd.read_csv('data/titanic.csv')
df_cleaned = clean_titanic_data(df_original)

# Sauvegarder
df_cleaned.to_csv('data/titanic_clean.csv', index=False)
print("Donn√©es nettoy√©es et sauvegard√©es !")
```

---

## ‚úÖ Checklist de nettoyage

Avant de commencer l'analyse, v√©rifiez :

- [ ] **Dimensions** : Nombre de lignes et colonnes coh√©rent ?
- [ ] **Types** : Chaque colonne a le bon type (int, float, str, datetime) ?
- [ ] **Valeurs manquantes** : Identifi√©es et trait√©es ?
- [ ] **Doublons** : V√©rifi√©s et supprim√©s si n√©cessaire ?
- [ ] **Outliers** : D√©tect√©s et trait√©s selon le contexte ?
- [ ] **Coh√©rence** : Valeurs dans les plages attendues ?
- [ ] **Encodage** : Variables cat√©gorielles encod√©es correctement ?
- [ ] **Features** : Nouvelles variables cr√©√©es si pertinent ?
- [ ] **Documentation** : Transformations document√©es ?
- [ ] **Sauvegarde** : Donn√©es propres sauvegard√©es ?

---

## üéì Bonnes pratiques

### 1. Toujours travailler sur une copie
```python
df_clean = df.copy()  # Ne jamais modifier l'original directement
```

### 2. Documenter les d√©cisions
```python
# Pourquoi cette transformation ?
# Age : Imputation par m√©diane car distribution asym√©trique
# Embarked : Mode car seulement 2 valeurs manquantes
```

### 3. Cr√©er des fonctions r√©utilisables
```python
def handle_missing_age(df):
    """Impute missing age values by median of Pclass and Sex"""
    return df.groupby(['Pclass', 'Sex'])['Age'].transform(
        lambda x: x.fillna(x.median())
    )
```

### 4. Valider les transformations
```python
# Avant
print(df['Age'].isnull().sum())  # 177
# Apr√®s
df['Age'] = handle_missing_age(df)
print(df['Age'].isnull().sum())  # 0
assert df['Age'].isnull().sum() == 0, "Des valeurs manquantes subsistent !"
```

### 5. Sauvegarder les √©tapes interm√©diaires
```python
df.to_csv('data/titanic_step1_missing_handled.csv', index=False)
df.to_csv('data/titanic_step2_features_created.csv', index=False)
df.to_csv('data/titanic_final_clean.csv', index=False)
```

---

## üìñ Ressources

- [pandas Missing Data](https://pandas.pydata.org/docs/user_guide/missing_data.html)
- [scikit-learn Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)
- [Feature Engineering Guide](https://www.kaggle.com/learn/feature-engineering)

---

## üîë Points cl√©s

1. **Diagnostic d'abord** : Comprendre les donn√©es avant de les modifier
2. **Contexte m√©tier** : Les d√©cisions de nettoyage d√©pendent du domaine
3. **Valeurs manquantes** : Plusieurs strat√©gies selon le type et le volume
4. **Outliers** : Ne pas supprimer syst√©matiquement, comprendre pourquoi
5. **Feature engineering** : Cr√©er de la valeur √† partir des donn√©es existantes
6. **Documentation** : Garder une trace de toutes les transformations
7. **Validation** : V√©rifier chaque √©tape de nettoyage

---

## üìù Pr√©paration Session 16

Dans la **Session 16**, nous ferons :
- Une EDA (Exploratory Data Analysis) compl√®te
- Des visualisations pour comprendre les relations
- Des analyses statistiques approfondies
- Des r√©ponses √† des questions m√©tier complexes

**Pr√©paration** :
- Terminez le nettoyage du dataset Titanic
- Installez matplotlib et seaborn : `pip install matplotlib seaborn`
