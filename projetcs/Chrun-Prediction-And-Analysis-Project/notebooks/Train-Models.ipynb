{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Notebook d'Entra√Ænement des Mod√®les de Pr√©diction du Churn\n",
    "\n",
    "## üéØ Objectif\n",
    "Ce notebook impl√©mente un pipeline complet de machine learning pour pr√©dire le churn (d√©sabonnement) des clients. Il couvre toutes les √©tapes essentielles :\n",
    "\n",
    "1. **Chargement et exploration des donn√©es**\n",
    "2. **Analyse exploratoire (EDA)**\n",
    "3. **Pr√©traitement et feature engineering**\n",
    "4. **Entra√Ænement de plusieurs mod√®les**\n",
    "5. **Optimisation des hyperparam√®tres**\n",
    "6. **√âvaluation et comparaison des mod√®les**\n",
    "7. **S√©lection et sauvegarde du meilleur mod√®le**\n",
    "\n",
    "## üìä Donn√©es\n",
    "Le dataset contient des informations sur les clients d'une entreprise de t√©l√©communications, incluant leurs caract√©ristiques d√©mographiques, les services souscrits, et leur statut de churn.\n",
    "\n",
    "## üéì Contexte Business\n",
    "Le churn (taux d'attrition client) est un indicateur critique pour les entreprises. Perdre des clients co√ªte g√©n√©ralement plus cher que d'en acqu√©rir de nouveaux. Ce mod√®le permet d'identifier proactivement les clients √† risque pour mettre en place des actions de r√©tention cibl√©es.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration et Imports\n",
    "\n",
    "Cette section initialise l'environnement de travail en important toutes les biblioth√®ques n√©cessaires pour :\n",
    "- La manipulation de donn√©es (pandas, numpy)\n",
    "- La visualisation (matplotlib, seaborn)\n",
    "- Le machine learning (scikit-learn)\n",
    "- La persistance des mod√®les (joblib)\n",
    "- Le logging des op√©rations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "# Pr√©traitement et mod√©lisation\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "# Mod√®les\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Sauvegarde des mod√®les et des m√©triques\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Ignorer les warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Organisation des Fichiers\n",
    "\n",
    "Cr√©ation d'une structure de dossiers pour organiser les artefacts du projet :\n",
    "- **`data/`** : Donn√©es interm√©diaires et transform√©es\n",
    "- **`figures/`** : Graphiques et visualisations\n",
    "- **`models/`** : Mod√®les entra√Æn√©s (.pkl)\n",
    "- **`metrics/`** : M√©triques d'√©valuation et rapports\n",
    "- **`logs/`** : Logs d'ex√©cution du projet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation des dossiers pour organiser les artefacts\n",
    "folders = ['data', 'figures', 'models', 'metrics', 'logs']\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Configuration du Logging\n",
    "\n",
    "Le syst√®me de logging permet de tracer toutes les op√©rations importantes du pipeline. Les logs sont enregistr√©s dans `logs/project_logs.log` pour faciliter le debugging et l'audit des entra√Ænements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du logger\n",
    "logging.basicConfig(\n",
    "    filename='logs/project_logs.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s:%(levelname)s:%(message)s'\n",
    ")\n",
    "\n",
    "logging.info('Initialisation du projet de pr√©diction du churn client.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Chargement et Exploration des Donn√©es\n",
    "\n",
    "### 2.1 Chargement des Donn√©es\n",
    "\n",
    "Les donn√©es sont charg√©es depuis le fichier CSV et des aper√ßus sont g√©n√©r√©s :\n",
    "- **`data.head()`** : Les 5 premi√®res lignes pour inspection rapide\n",
    "- **`data.info()`** : Types de donn√©es et valeurs manquantes\n",
    "- **`data.describe()`** : Statistiques descriptives des variables num√©riques\n",
    "\n",
    "Ces exports permettent une analyse rapide sans avoir √† relancer le notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Aper√ßu des donn√©es:\n",
      "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
      "0  7590-VHVEG  Female              0     Yes         No       1           No   \n",
      "1  5575-GNVDE    Male              0      No         No      34          Yes   \n",
      "2  3668-QPYBK    Male              0      No         No       2          Yes   \n",
      "3  7795-CFOCW    Male              0      No         No      45           No   \n",
      "4  9237-HQITU  Female              0      No         No       2          Yes   \n",
      "\n",
      "      MultipleLines InternetService OnlineSecurity  ... DeviceProtection  \\\n",
      "0  No phone service             DSL             No  ...               No   \n",
      "1                No             DSL            Yes  ...              Yes   \n",
      "2                No             DSL            Yes  ...               No   \n",
      "3  No phone service             DSL            Yes  ...              Yes   \n",
      "4                No     Fiber optic             No  ...               No   \n",
      "\n",
      "  TechSupport StreamingTV StreamingMovies        Contract PaperlessBilling  \\\n",
      "0          No          No              No  Month-to-month              Yes   \n",
      "1          No          No              No        One year               No   \n",
      "2          No          No              No  Month-to-month              Yes   \n",
      "3         Yes          No              No        One year               No   \n",
      "4          No          No              No  Month-to-month              Yes   \n",
      "\n",
      "               PaymentMethod MonthlyCharges  TotalCharges Churn  \n",
      "0           Electronic check          29.85         29.85    No  \n",
      "1               Mailed check          56.95        1889.5    No  \n",
      "2               Mailed check          53.85        108.15   Yes  \n",
      "3  Bank transfer (automatic)          42.30       1840.75    No  \n",
      "4           Electronic check          70.70        151.65   Yes  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "‚úÖ Donn√©es charg√©es avec succ√®s: 7043 lignes et 21 colonnes\n",
      "\n",
      "üìà Statistiques descriptives des variables num√©riques:\n",
      "       SeniorCitizen       tenure  MonthlyCharges\n",
      "count    7043.000000  7043.000000     7043.000000\n",
      "mean        0.162147    32.371149       64.761692\n",
      "std         0.368612    24.559481       30.090047\n",
      "min         0.000000     0.000000       18.250000\n",
      "25%         0.000000     9.000000       35.500000\n",
      "50%         0.000000    29.000000       70.350000\n",
      "75%         0.000000    55.000000       89.850000\n",
      "max         1.000000    72.000000      118.750000\n",
      "\n",
      "üîç Valeurs manquantes par colonne:\n",
      "Aucune valeur manquante d√©tect√©e\n"
     ]
    }
   ],
   "source": [
    "# Chargement des donn√©es depuis un fichier CSV\n",
    "data = pd.read_csv('../data/data.csv')\n",
    "\n",
    "# Affichage des premi√®res lignes pour un aper√ßu rapide\n",
    "print(\"üìä Aper√ßu des donn√©es:\")\n",
    "print(data.head())\n",
    "print(f\"\\n‚úÖ Donn√©es charg√©es avec succ√®s: {data.shape[0]} lignes et {data.shape[1]} colonnes\")\n",
    "\n",
    "# Enregistrement de l'aper√ßu des donn√©es\n",
    "data.head().to_csv('data/data_preview.csv', index=False)\n",
    "logging.info('Aper√ßu des donn√©es enregistr√© dans data/data_preview.csv')\n",
    "\n",
    "# Informations sur le DataFrame\n",
    "with open('data/data_info.txt', 'w') as f:\n",
    "    data.info(buf=f)\n",
    "logging.info('Informations sur les donn√©es enregistr√©es dans data/data_info.txt')\n",
    "\n",
    "# Statistiques descriptives\n",
    "print(\"\\nüìà Statistiques descriptives des variables num√©riques:\")\n",
    "print(data.describe())\n",
    "\n",
    "# Enregistrement des statistiques descriptives\n",
    "data.describe().to_csv('data/data_describe.csv')\n",
    "logging.info('Statistiques descriptives enregistr√©es dans data/data_describe.csv')\n",
    "\n",
    "# V√©rification des valeurs manquantes\n",
    "print(\"\\nüîç Valeurs manquantes par colonne:\")\n",
    "missing_values = data.isnull().sum()\n",
    "if missing_values.sum() > 0:\n",
    "    print(missing_values[missing_values > 0])\n",
    "else:\n",
    "    print(\"Aucune valeur manquante d√©tect√©e\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Nettoyage des Donn√©es\n",
    "\n",
    "**Traitement de la variable `TotalCharges` :**\n",
    "- Certaines valeurs peuvent √™tre stock√©es comme texte au lieu de nombres\n",
    "- Conversion en num√©rique avec gestion des erreurs (`errors='coerce'`)\n",
    "- Imputation des valeurs manquantes par la m√©diane (robuste aux outliers)\n",
    "\n",
    "**Encodage de la variable cible :**\n",
    "- Transformation de `Churn` (Yes/No) en variable binaire (1/0)\n",
    "- Facilite l'utilisation dans les algorithmes de machine learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Nettoyage de la colonne 'TotalCharges'...\n",
      "‚ö†Ô∏è  11 valeurs non num√©riques converties en NaN\n",
      "üìä Imputation par la m√©diane: 1397.47\n",
      "\n",
      "üéØ Distribution de la variable cible:\n",
      "  - Clients non-churned (No): 5174 (73.5%)\n",
      "  - Clients churned (Yes): 1869 (26.5%)\n"
     ]
    }
   ],
   "source": [
    "# Nettoyage de 'TotalCharges' si n√©cessaire\n",
    "# Certaines valeurs peuvent √™tre stock√©es comme string au lieu de num√©rique\n",
    "print(\"üßπ Nettoyage de la colonne 'TotalCharges'...\")\n",
    "\n",
    "# Conversion en num√©rique avec gestion des erreurs\n",
    "data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\n",
    "\n",
    "# Affichage du nombre de valeurs manquantes cr√©√©es\n",
    "n_missing = data['TotalCharges'].isnull().sum()\n",
    "if n_missing > 0:\n",
    "    print(f\"‚ö†Ô∏è  {n_missing} valeurs non num√©riques converties en NaN\")\n",
    "    print(f\"üìä Imputation par la m√©diane: {data['TotalCharges'].median():.2f}\")\n",
    "    \n",
    "# Imputation des valeurs manquantes par la m√©diane (robuste aux outliers)\n",
    "data['TotalCharges'].fillna(data['TotalCharges'].median(), inplace=True)\n",
    "\n",
    "# Encodage de la variable cible\n",
    "data['Churn_encoded'] = data['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Affichage de la distribution du churn\n",
    "print(f\"\\nüéØ Distribution de la variable cible:\")\n",
    "print(f\"  - Clients non-churned (No): {(data['Churn'] == 'No').sum()} ({(data['Churn'] == 'No').sum() / len(data) * 100:.1f}%)\")\n",
    "print(f\"  - Clients churned (Yes): {(data['Churn'] == 'Yes').sum()} ({(data['Churn'] == 'Yes').sum() / len(data) * 100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Analyse Exploratoire des Donn√©es (EDA)\n",
    "\n",
    "L'EDA est une √©tape cruciale pour comprendre les donn√©es et identifier les patterns li√©s au churn.\n",
    "\n",
    "### 3.1 Analyse des Variables Cat√©goriques\n",
    "\n",
    "**Objectif :** Visualiser la distribution des variables cat√©goriques en fonction du statut de churn.\n",
    "\n",
    "**Insights attendus :**\n",
    "- Quels services ou caract√©ristiques sont associ√©s √† un taux de churn plus √©lev√© ?\n",
    "- Y a-t-il des d√©s√©quilibres importants dans certaines cat√©gories ?\n",
    "- Les contrats √† long terme r√©duisent-ils le churn ?\n",
    "\n",
    "Les barplots permettent de comparer visuellement les proportions de churn pour chaque cat√©gorie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des variables cat√©goriques\n",
    "categorical_vars = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService',\n",
    "                    'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',\n",
    "                    'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n",
    "                    'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
    "\n",
    "# Param√®tres pour les sous-graphiques\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(categorical_vars) / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, var in enumerate(categorical_vars):\n",
    "    sns.countplot(x=var, hue='Churn', data=data, ax=axes[idx])\n",
    "    axes[idx].set_title(f'Distribution de {var} par Churn')\n",
    "    axes[idx].set_xlabel(var)\n",
    "    axes[idx].set_ylabel('Nombre')\n",
    "    axes[idx].legend(title='Churn', loc='upper right')\n",
    "\n",
    "# Supprimer les axes vides\n",
    "for ax in axes[len(categorical_vars):]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/categorical_barplots.png')\n",
    "plt.close()\n",
    "logging.info('Barplots des variables cat√©goriques enregistr√©s dans figures/categorical_barplots.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Analyse des Variables Num√©riques\n",
    "\n",
    "**Variables analys√©es :**\n",
    "- **`tenure`** : Anciennet√© du client (en mois) - Les clients r√©cents sont-ils plus √† risque ?\n",
    "- **`MonthlyCharges`** : Frais mensuels - Un co√ªt √©lev√© augmente-t-il le churn ?\n",
    "- **`TotalCharges`** : Charges totales cumul√©es - Corr√©lation avec la fid√©lit√© ?\n",
    "\n",
    "Les boxplots r√©v√®lent les diff√©rences de distribution entre clients churned et non-churned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables num√©riques\n",
    "numerical_vars = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(numerical_vars) / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, var in enumerate(numerical_vars):\n",
    "    sns.boxplot(x='Churn', y=var, data=data, ax=axes[idx])\n",
    "    axes[idx].set_title(f'Boxplot de {var} par Churn')\n",
    "    axes[idx].set_xlabel('Churn')\n",
    "    axes[idx].set_ylabel(var)\n",
    "\n",
    "# Supprimer les axes vides\n",
    "for ax in axes[len(numerical_vars):]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/numerical_boxplots.png')\n",
    "plt.close()\n",
    "logging.info('Boxplots des variables num√©riques enregistr√©s dans figures/numerical_boxplots.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Matrice de Corr√©lation\n",
    "\n",
    "**Analyse des relations lin√©aires** entre les variables num√©riques et la variable cible.\n",
    "\n",
    "**Points d'attention :**\n",
    "- Multicolin√©arit√© entre variables (corr√©lation > 0.7-0.8)\n",
    "- Corr√©lation avec la variable cible (`Churn_encoded`)\n",
    "- TotalCharges et tenure sont souvent fortement corr√©l√©s (logique : plus on reste, plus on paie)\n",
    "\n",
    "Cette analyse guide les d√©cisions de feature engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de corr√©lation\n",
    "corr_matrix = data[numerical_vars + ['Churn_encoded']].corr()\n",
    "\n",
    "# Heatmap de la matrice de corr√©lation\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Matrice de Corr√©lation')\n",
    "plt.savefig('figures/correlation_matrix.png')\n",
    "plt.close()\n",
    "logging.info('Matrice de corr√©lation enregistr√©e dans figures/correlation_matrix.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Pr√©traitement et Feature Engineering\n",
    "\n",
    "### 4.1 S√©paration Features / Target\n",
    "\n",
    "**Exclusions :**\n",
    "- **`Churn`** : Variable cible (version texte)\n",
    "- **`customerID`** : Identifiant unique, non informatif pour la pr√©diction\n",
    "- **`Churn_encoded`** : Variable cible (version num√©rique)\n",
    "\n",
    "**Cat√©gorisation des variables :**\n",
    "1. **Variables binaires** : 2 valeurs uniques (ex: gender, Partner)\n",
    "2. **Variables multi-cat√©gorielles** : Plus de 2 valeurs (ex: InternetService, Contract)\n",
    "3. **Variables num√©riques** : D√©j√† identifi√©es (tenure, charges)\n",
    "\n",
    "Cette cat√©gorisation permet d'appliquer le bon type d'encodage √† chaque type de variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©paration des features et de la cible\n",
    "X = data.drop(['Churn', 'customerID', 'Churn_encoded'], axis=1)\n",
    "y = data['Churn_encoded']\n",
    "\n",
    "# Identification des variables binaires et cat√©goriques\n",
    "binary_vars = [col for col in X.columns if X[col].nunique() == 2]\n",
    "multi_category_vars = [col for col in X.columns if X[col].dtype == 'object' and col not in binary_vars]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Pipeline de Pr√©traitement\n",
    "\n",
    "Utilisation de **`ColumnTransformer`** de scikit-learn pour appliquer diff√©rentes transformations en parall√®le :\n",
    "\n",
    "1. **Variables num√©riques ‚Üí StandardScaler**\n",
    "   - Centrage (moyenne = 0) et r√©duction (√©cart-type = 1)\n",
    "   - Important pour les mod√®les sensibles √† l'√©chelle (SVM, r√©gression logistique)\n",
    "\n",
    "2. **Variables binaires ‚Üí OrdinalEncoder**\n",
    "   - Transformation simple en 0/1\n",
    "   - Pr√©serve la nature binaire\n",
    "\n",
    "3. **Variables cat√©gorielles ‚Üí OneHotEncoder**\n",
    "   - Cr√©e une colonne binaire par cat√©gorie\n",
    "   - `drop='first'` : √©vite la multicolin√©arit√© parfaite\n",
    "   - Exemple : InternetService ‚Üí [DSL, Fiber optic] (No est la r√©f√©rence)\n",
    "\n",
    "**Avantage :** Pipeline r√©utilisable et garantit la coh√©rence entre train/validation/test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation du pr√©processeur\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_vars),\n",
    "        ('bin', OrdinalEncoder(), binary_vars),\n",
    "        ('cat', OneHotEncoder(drop='first'), multi_category_vars)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Application du Pr√©traitement\n",
    "\n",
    "Application du pr√©processeur sur l'ensemble complet des features pour :\n",
    "- V√©rifier que la transformation fonctionne correctement\n",
    "- Obtenir les noms des features apr√®s encodage\n",
    "- Sauvegarder un aper√ßu des donn√©es transform√©es\n",
    "\n",
    "**Note :** Dans le pipeline final, la transformation sera appliqu√©e s√©par√©ment sur train/val/test pour √©viter le data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application du pr√©processeur sur l'ensemble des donn√©es\n",
    "X_normalized = preprocessor.fit_transform(X)\n",
    "\n",
    "# R√©cup√©ration des noms de colonnes apr√®s encodage\n",
    "num_features = numerical_vars\n",
    "bin_features = binary_vars\n",
    "cat_features = preprocessor.named_transformers_['cat'].get_feature_names_out(multi_category_vars)\n",
    "all_features = np.concatenate([num_features, bin_features, cat_features])\n",
    "\n",
    "# Conversion en DataFrame\n",
    "X_normalized_df = pd.DataFrame(X_normalized, columns=all_features)\n",
    "\n",
    "# Enregistrement de la DataFrame normalis√©e\n",
    "X_normalized_df.to_csv('data/X_normalized.csv', index=False)\n",
    "logging.info('DataFrame normalis√©e enregistr√©e dans data/X_normalized.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Division des Donn√©es (Train / Validation / Test)\n",
    "\n",
    "**Strat√©gie de split :**\n",
    "- **70% Train** : Entra√Ænement des mod√®les\n",
    "- **15% Validation** : Optimisation des hyperparam√®tres et s√©lection du mod√®le\n",
    "- **15% Test** : √âvaluation finale du mod√®le s√©lectionn√© (non utilis√© dans ce notebook)\n",
    "\n",
    "**`stratify=y`** : Pr√©serve la proportion de churn dans chaque ensemble (important car le churn est souvent d√©s√©quilibr√©).\n",
    "\n",
    "**Pourquoi 3 ensembles ?**\n",
    "- √âvite l'overfitting sur les donn√©es de validation\n",
    "- Le test set donne une estimation non biais√©e des performances en production\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Division des donn√©es:\n",
      "  - Entra√Ænement: 4930 samples (70.0%)\n",
      "  - Validation: 1056 samples (15.0%)\n",
      "  - Test: 1057 samples (15.0%)\n",
      "\n",
      "üéØ Distribution du churn:\n",
      "  - Train: 1308/4930 churned (26.5%)\n",
      "  - Validation: 280/1056 churned (26.5%)\n",
      "  - Test: 281/1057 churned (26.6%)\n"
     ]
    }
   ],
   "source": [
    "# Division initiale en entra√Ænement et temporaire (70% entra√Ænement, 30% temp)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n",
    "\n",
    "# Division du temporaire en validation et test (50% validation, 50% test)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Affichage des tailles et distribution\n",
    "print(\"üìä Division des donn√©es:\")\n",
    "print(f\"  - Entra√Ænement: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Validation: {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Test: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüéØ Distribution du churn:\")\n",
    "print(f\"  - Train: {y_train.sum()}/{len(y_train)} churned ({y_train.mean()*100:.1f}%)\")\n",
    "print(f\"  - Validation: {y_val.sum()}/{len(y_val)} churned ({y_val.mean()*100:.1f}%)\")\n",
    "print(f\"  - Test: {y_test.sum()}/{len(y_test)} churned ({y_test.mean()*100:.1f}%)\")\n",
    "\n",
    "# Logging\n",
    "logging.info(f\"Taille de l'entra√Ænement: {X_train.shape}\")\n",
    "logging.info(f\"Taille de la validation: {X_val.shape}\")\n",
    "logging.info(f\"Taille du test: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Entra√Ænement des Mod√®les\n",
    "\n",
    "### 5.1 Cr√©ation des Pipelines\n",
    "\n",
    "Chaque pipeline combine le pr√©traitement et un algorithme de machine learning :\n",
    "\n",
    "**1. R√©gression Logistique**\n",
    "- Mod√®le lin√©aire simple et interpr√©table\n",
    "- `max_iter=1000` : Augmente les it√©rations pour convergence\n",
    "- Baseline solide pour probl√®mes de classification binaire\n",
    "\n",
    "**2. Random Forest**\n",
    "- Ensemble de arbres de d√©cision\n",
    "- G√®re bien les non-lin√©arit√©s et interactions\n",
    "- Moins sensible aux outliers\n",
    "\n",
    "**3. Support Vector Machine (SVM)**\n",
    "- Trouve la fronti√®re optimale entre classes\n",
    "- `probability=True` : Active les probabilit√©s (n√©cessaire pour ROC-AUC)\n",
    "- Peut capturer des patterns complexes avec les noyaux\n",
    "\n",
    "**Avantage des pipelines :** Garantit que le pr√©traitement est appliqu√© de mani√®re identique √† chaque √©tape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines pour les mod√®les sans hyperparam√®tres\n",
    "pipeline_lr = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "pipeline_svc = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "logging.info('Pipelines scikit-learn cr√©√©s pour chaque mod√®le.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Grilles d'Hyperparam√®tres\n",
    "\n",
    "D√©finition des hyperparam√®tres √† tester pour chaque mod√®le via GridSearchCV.\n",
    "\n",
    "**R√©gression Logistique :**\n",
    "- **`C`** : Inverse de la r√©gularisation (‚ÜëC = ‚Üìr√©gularisation = mod√®le plus complexe)\n",
    "- **`penalty='l2'`** : R√©gularisation Ridge (p√©nalise les coefficients √©lev√©s)\n",
    "\n",
    "**Random Forest :**\n",
    "- **`n_estimators`** : Nombre d'arbres (‚Üë = plus stable mais plus lent)\n",
    "- **`max_depth`** : Profondeur maximale (contr√¥le l'overfitting)\n",
    "- **`min_samples_split/leaf`** : Crit√®res d'arr√™t pour √©viter l'overfitting\n",
    "\n",
    "**SVM :**\n",
    "- **`C`** : Contr√¥le le compromis biais/variance\n",
    "- **`kernel`** : Type de fronti√®re (linear = lin√©aire, rbf = non-lin√©aire)\n",
    "- **`gamma`** : Influence du kernel RBF (‚Üëgamma = plus de complexit√©)\n",
    "\n",
    "**Note :** GridSearchCV testera toutes les combinaisons pour trouver la meilleure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param√®tres pour la R√©gression Logistique\n",
    "param_grid_lr = {\n",
    "    'classifier__C': [0.01, 0.1, 1, 10],\n",
    "    'classifier__penalty': ['l2'],\n",
    "    'classifier__solver': ['lbfgs']\n",
    "}\n",
    "\n",
    "# Param√®tres pour la For√™t Al√©atoire\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [None, 10, 20],\n",
    "    'classifier__min_samples_split': [2, 5],\n",
    "    'classifier__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Param√®tres pour le SVM\n",
    "param_grid_svc = {\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__kernel': ['rbf', 'linear'],\n",
    "    'classifier__gamma': ['scale', 'auto']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Fonction d'Optimisation\n",
    "\n",
    "**GridSearchCV** effectue :\n",
    "1. **Validation crois√©e 5-fold** : Divise train en 5 parties, entra√Æne sur 4, valide sur 1 (rotation)\n",
    "2. **Optimisation sur ROC-AUC** : M√©trique robuste pour donn√©es d√©s√©quilibr√©es\n",
    "3. **Parall√©lisation** (`n_jobs=-1`) : Utilise tous les CPU disponibles\n",
    "\n",
    "**Pourquoi ROC-AUC ?**\n",
    "- Insensible au d√©s√©quilibre des classes\n",
    "- √âvalue la qualit√© du ranking des pr√©dictions\n",
    "- AUC=0.5 : mod√®le al√©atoire / AUC=1.0 : mod√®le parfait\n",
    "\n",
    "La fonction retourne le meilleur estimateur (mod√®le + hyperparam√®tres optimaux).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(pipeline, param_grid, X_train, y_train, model_name):\n",
    "    logging.info(f\"D√©but du fine-tuning pour {model_name}\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        scoring='roc_auc',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    logging.info(f\"Meilleurs param√®tres pour {model_name}: {grid_search.best_params_}\")\n",
    "    return grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Entra√Ænement et Optimisation\n",
    "\n",
    "Lancement du fine-tuning pour les 3 mod√®les en parall√®le.\n",
    "\n",
    "**Processus :**\n",
    "- Pour chaque combinaison d'hyperparam√®tres\n",
    "- GridSearch entra√Æne le mod√®le 5 fois (5-fold CV)\n",
    "- Calcule le ROC-AUC moyen\n",
    "- S√©lectionne la meilleure configuration\n",
    "\n",
    "**Nombre total d'entra√Ænements :**\n",
    "- Logistic Regression : 4 combinaisons √ó 5 folds = 20 fits\n",
    "- Random Forest : 24 combinaisons √ó 5 folds = 120 fits  \n",
    "- SVM : 12 combinaisons √ó 5 folds = 60 fits\n",
    "\n",
    "‚è±Ô∏è **Note :** Cette cellule peut prendre plusieurs minutes selon la puissance de calcul.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning pour la R√©gression Logistique\n",
    "best_model_lr = fine_tune_model(pipeline_lr, param_grid_lr, X_train, y_train, 'LogisticRegression')\n",
    "\n",
    "# Fine-tuning pour la For√™t Al√©atoire\n",
    "best_model_rf = fine_tune_model(pipeline_rf, param_grid_rf, X_train, y_train, 'RandomForest')\n",
    "\n",
    "# Fine-tuning pour le SVM\n",
    "best_model_svc = fine_tune_model(pipeline_svc, param_grid_svc, X_train, y_train, 'SVC')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. √âvaluation des Mod√®les\n",
    "\n",
    "### 6.1 Fonction de Visualisation de la Matrice de Confusion\n",
    "\n",
    "La matrice de confusion affiche :\n",
    "- **Vrais N√©gatifs (TN)** : Clients correctement pr√©dits comme non-churned\n",
    "- **Faux Positifs (FP)** : Clients non-churned pr√©dits comme churned (Type I)\n",
    "- **Faux N√©gatifs (FN)** : Clients churned pr√©dits comme non-churned (Type II - ‚ö†Ô∏è critique !)\n",
    "- **Vrais Positifs (TP)** : Clients correctement pr√©dits comme churned\n",
    "\n",
    "Les pourcentages facilitent l'interpr√©tation des performances par classe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names, title='Confusion matrix', cmap=plt.cm.Blues, file_name='confusion_matrix.png'):\n",
    "    \"\"\"\n",
    "    Trace la matrice de confusion personnalis√©e.\n",
    "\n",
    "    Args:\n",
    "        cm (np.ndarray): Matrice de confusion.\n",
    "        class_names (list): Liste des noms des classes.\n",
    "        title (str): Titre du graphique.\n",
    "        cmap (matplotlib.colors.Colormap): Colormap pour le graphique.\n",
    "        file_name (str): Nom du fichier pour enregistrer le graphique.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Plotting confusion matrix for {title}\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        percentage = 100 * cm[i, j] / cm[i, :].sum() if cm[i, :].sum() != 0 else 0\n",
    "        plt.text(j, i, f'{cm[i, j]} ({percentage:.2f}%)',\n",
    "                 horizontalalignment=\"center\", verticalalignment='center',\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(file_name)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Fonction d'√âvaluation Compl√®te\n",
    "\n",
    "Pour chaque mod√®le, calcule et sauvegarde :\n",
    "\n",
    "**1. M√©triques de classification**\n",
    "- **Pr√©cision (Precision)** : Parmi les pr√©dictions positives, combien sont correctes ?\n",
    "  - Important si le co√ªt d'un FP est √©lev√©\n",
    "- **Rappel (Recall)** : Parmi les vrais positifs, combien sont d√©tect√©s ?\n",
    "  - Critique pour le churn : ne pas rater de clients √† risque\n",
    "- **F1-Score** : Moyenne harmonique de pr√©cision et rappel\n",
    "  - √âquilibre entre les deux\n",
    "\n",
    "**2. Courbe ROC et AUC**\n",
    "- Visualise le compromis entre TPR (True Positive Rate) et FPR (False Positive Rate)\n",
    "- AUC r√©sume la performance en un seul chiffre\n",
    "\n",
    "**3. Sauvegarde des artefacts**\n",
    "- Rapports texte pour revue d√©taill√©e\n",
    "- Graphiques pour pr√©sentation\n",
    "- M√©triques JSON pour tracking MLOps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_val, y_val, model_name):\n",
    "    logging.info(f'√âvaluation du mod√®le {model_name}')\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_proba = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Calcul des m√©triques\n",
    "    report = classification_report(y_val, y_pred, output_dict=True)\n",
    "    auc = roc_auc_score(y_val, y_proba)\n",
    "    fpr, tpr, thresholds = roc_curve(y_val, y_proba)\n",
    "    \n",
    "    # Enregistrement du rapport de classification\n",
    "    with open(f'metrics/classification_report_{model_name}.txt', 'w') as f:\n",
    "        f.write(classification_report(y_val, y_pred))\n",
    "    logging.info(f'Rapport de classification enregistr√© pour {model_name}')\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    class_names = ['No', 'Yes']\n",
    "    plot_confusion_matrix(cm, class_names, title=f'Matrice de Confusion - {model_name}', file_name=f'figures/confusion_matrix_{model_name}.png')\n",
    "    \n",
    "    # Courbe ROC\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.title(f'Courbe ROC - {model_name}')\n",
    "    plt.xlabel('Taux de Faux Positifs')\n",
    "    plt.ylabel('Taux de Vrais Positifs')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(f'figures/roc_curve_{model_name}.png')\n",
    "    plt.close()\n",
    "    logging.info(f'Courbe ROC enregistr√©e pour {model_name}')\n",
    "    \n",
    "    # Sauvegarder les m√©triques\n",
    "    metrics = {\n",
    "        'classification_report': report,\n",
    "        'auc_roc': auc\n",
    "    }\n",
    "    with open(f'metrics/metrics_{model_name}.json', 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    logging.info(f'M√©triques enregistr√©es pour {model_name}')\n",
    "    \n",
    "    return report, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 √âvaluation sur l'Ensemble de Validation\n",
    "\n",
    "Application de la fonction d'√©valuation aux 3 mod√®les optimis√©s.\n",
    "\n",
    "Les r√©sultats sont stock√©s dans :\n",
    "- `models_reports` : Rapports de classification d√©taill√©s\n",
    "- `models_auc` : Scores AUC-ROC pour chaque mod√®le\n",
    "\n",
    "Ces m√©triques permettront de comparer objectivement les mod√®les et de s√©lectionner le meilleur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaires pour stocker les rapports et les AUC\n",
    "models_reports = {}\n",
    "models_auc = {}\n",
    "\n",
    "# √âvaluation du mod√®le Logistic Regression\n",
    "report_lr, auc_lr = evaluate_model(best_model_lr, X_val, y_val, 'LogisticRegression')\n",
    "models_reports['LogisticRegression'] = report_lr\n",
    "models_auc['LogisticRegression'] = auc_lr\n",
    "\n",
    "# √âvaluation du mod√®le Random Forest\n",
    "report_rf, auc_rf = evaluate_model(best_model_rf, X_val, y_val, 'RandomForest')\n",
    "models_reports['RandomForest'] = report_rf\n",
    "models_auc['RandomForest'] = auc_rf\n",
    "\n",
    "# √âvaluation du mod√®le SVC\n",
    "report_svc, auc_svc = evaluate_model(best_model_svc, X_val, y_val, 'SVC')\n",
    "models_reports['SVC'] = report_svc\n",
    "models_auc['SVC'] = auc_svc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Sauvegarde des Mod√®les\n",
    "\n",
    "Persistance des 3 mod√®les optimis√©s avec `joblib` :\n",
    "- Format efficace pour les objets Python/scikit-learn\n",
    "- Permet le chargement rapide pour l'inf√©rence\n",
    "- Inclut le pipeline complet (pr√©traitement + mod√®le)\n",
    "\n",
    "Ces fichiers `.pkl` peuvent √™tre d√©ploy√©s directement en production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Sauvegarde des mod√®les...\n",
      "  ‚úÖ Logistic Regression sauvegard√©\n",
      "  ‚úÖ Random Forest sauvegard√©\n",
      "  ‚úÖ SVM sauvegard√©\n",
      "\n",
      "‚ú® Tous les mod√®les ont √©t√© sauvegard√©s avec succ√®s dans le dossier 'models/'\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde des mod√®les optimis√©s\n",
    "print(\"üíæ Sauvegarde des mod√®les...\")\n",
    "\n",
    "joblib.dump(best_model_lr, 'models/model_LogisticRegression.pkl')\n",
    "print(\"  ‚úÖ Logistic Regression sauvegard√©\")\n",
    "\n",
    "joblib.dump(best_model_rf, 'models/model_RandomForest.pkl')\n",
    "print(\"  ‚úÖ Random Forest sauvegard√©\")\n",
    "\n",
    "joblib.dump(best_model_svc, 'models/model_SVC.pkl')\n",
    "print(\"  ‚úÖ SVM sauvegard√©\")\n",
    "\n",
    "logging.info('Mod√®les optimis√©s sauvegard√©s dans le dossier models.')\n",
    "print(\"\\n‚ú® Tous les mod√®les ont √©t√© sauvegard√©s avec succ√®s dans le dossier 'models/'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Comparaison et S√©lection du Meilleur Mod√®le\n",
    "\n",
    "### 7.1 Consolidation des Rapports\n",
    "\n",
    "Agr√©gation de tous les rapports de classification dans un DataFrame unique.\n",
    "\n",
    "Facilite la comparaison c√¥te √† c√¥te des performances de chaque mod√®le sur toutes les m√©triques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation d'une liste pour stocker les rapports\n",
    "classification_reports = []\n",
    "\n",
    "for model_name, report in models_reports.items():\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    report_df['model'] = model_name\n",
    "    report_df['metric'] = report_df.index\n",
    "    classification_reports.append(report_df)\n",
    "\n",
    "# Concat√©ner tous les rapports\n",
    "all_reports_df = pd.concat(classification_reports, axis=0)\n",
    "\n",
    "# R√©organisation des colonnes\n",
    "cols = ['model', 'metric'] + [col for col in all_reports_df.columns if col not in ['model', 'metric']]\n",
    "all_reports_df = all_reports_df[cols]\n",
    "\n",
    "# Enregistrement dans un fichier CSV\n",
    "all_reports_df.to_csv('metrics/all_classification_reports.csv', index=False)\n",
    "logging.info('Tous les rapports de classification enregistr√©s dans metrics/all_classification_reports.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Benchmarking et S√©lection\n",
    "\n",
    "**M√©thode de scoring pond√©r√© :**\n",
    "- Chaque m√©trique re√ßoit un poids √©gal (25%)\n",
    "- Score final = moyenne pond√©r√©e des 4 m√©triques cl√©s\n",
    "\n",
    "**‚ö†Ô∏è Note :** Dans un contexte business r√©el, les poids devraient refl√©ter les priorit√©s m√©tier :\n",
    "- Si le co√ªt de manquer un churner est √©lev√© ‚Üí augmenter le poids du recall\n",
    "- Si les actions de r√©tention sont co√ªteuses ‚Üí privil√©gier la precision\n",
    "\n",
    "**Classe positive ('1') :** Le scoring se base uniquement sur la classe churn, car c'est la classe d'int√©r√™t.\n",
    "\n",
    "Le mod√®le avec le score final le plus √©lev√© est s√©lectionn√© comme meilleur mod√®le.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Calcul des scores finaux pond√©r√©s...\n",
      "\n",
      "LogisticRegression:\n",
      "  Precision: 0.6471\n",
      "  Recall: 0.5893\n",
      "  F1-Score: 0.6168\n",
      "  AUC-ROC: 0.8452\n",
      "  üìà Score Final: 0.6746\n",
      "\n",
      "RandomForest:\n",
      "  Precision: 0.6364\n",
      "  Recall: 0.5250\n",
      "  F1-Score: 0.5753\n",
      "  AUC-ROC: 0.8423\n",
      "  üìà Score Final: 0.6448\n",
      "\n",
      "SVC:\n",
      "  Precision: 0.6403\n",
      "  Recall: 0.5786\n",
      "  F1-Score: 0.6079\n",
      "  AUC-ROC: 0.8375\n",
      "  üìà Score Final: 0.6661\n",
      "\n",
      "============================================================\n",
      "üèÜ MEILLEUR MOD√àLE: LogisticRegression\n",
      "üìä Score Final: 0.6746\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Poids des m√©triques pour le scoring final\n",
    "# Ajustez ces poids selon vos priorit√©s business\n",
    "weights = {\n",
    "    'precision': 0.25,  # Importance de minimiser les faux positifs\n",
    "    'recall': 0.25,     # Importance de d√©tecter tous les churners\n",
    "    'f1-score': 0.25,   # √âquilibre entre pr√©cision et rappel\n",
    "    'auc_roc': 0.25     # Performance globale du classifieur\n",
    "}\n",
    "\n",
    "benchmark_results = {}\n",
    "\n",
    "print(\"üìä Calcul des scores finaux pond√©r√©s...\\n\")\n",
    "\n",
    "for model_name, report in models_reports.items():\n",
    "    # R√©cup√©rer les scores pour la classe positive ('1')\n",
    "    metrics_class_1 = report['1']\n",
    "    auc = models_auc[model_name]\n",
    "    \n",
    "    # Calcul du score final pond√©r√©\n",
    "    final_score = (\n",
    "        weights['precision'] * metrics_class_1['precision'] +\n",
    "        weights['recall'] * metrics_class_1['recall'] +\n",
    "        weights['f1-score'] * metrics_class_1['f1-score'] +\n",
    "        weights['auc_roc'] * auc\n",
    "    )\n",
    "    \n",
    "    benchmark_results[model_name] = {\n",
    "        'precision': metrics_class_1['precision'],\n",
    "        'recall': metrics_class_1['recall'],\n",
    "        'f1-score': metrics_class_1['f1-score'],\n",
    "        'auc_roc': auc,\n",
    "        'final_score': final_score\n",
    "    }\n",
    "    \n",
    "    # Affichage des r√©sultats\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Precision: {metrics_class_1['precision']:.4f}\")\n",
    "    print(f\"  Recall: {metrics_class_1['recall']:.4f}\")\n",
    "    print(f\"  F1-Score: {metrics_class_1['f1-score']:.4f}\")\n",
    "    print(f\"  AUC-ROC: {auc:.4f}\")\n",
    "    print(f\"  üìà Score Final: {final_score:.4f}\\n\")\n",
    "\n",
    "# Cr√©ation de la DataFrame des r√©sultats\n",
    "benchmark_df = pd.DataFrame(benchmark_results).T\n",
    "\n",
    "# Enregistrement de la DataFrame des scores\n",
    "benchmark_df.to_csv('metrics/benchmark_results.csv')\n",
    "logging.info('R√©sultats du benchmarking enregistr√©s dans metrics/benchmark_results.csv')\n",
    "\n",
    "# Affichage du meilleur mod√®le\n",
    "best_model_name = benchmark_df['final_score'].idxmax()\n",
    "best_score = benchmark_df.loc[best_model_name, 'final_score']\n",
    "print(\"=\" * 60)\n",
    "print(f\"üèÜ MEILLEUR MOD√àLE: {best_model_name}\")\n",
    "print(f\"üìä Score Final: {best_score:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "logging.info(f'Le meilleur mod√®le est : {best_model_name} (score: {best_score:.4f})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Visualisations Comparatives\n",
    "\n",
    "**1. Comparaison multi-m√©triques**\n",
    "- Barplot group√© montrant les 4 m√©triques pour chaque mod√®le\n",
    "- Permet d'identifier visuellement les forces/faiblesses de chaque approche\n",
    "\n",
    "**2. Score final pond√©r√©**\n",
    "- Barplot simple du score agr√©g√©\n",
    "- Conclusion visuelle pour la s√©lection du mod√®le\n",
    "\n",
    "Ces visualisations facilitent la communication des r√©sultats aux parties prenantes non-techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des mod√®les sur les m√©triques\n",
    "benchmark_df[['precision', 'recall', 'f1-score', 'auc_roc']].plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Comparaison des Mod√®les sur les Diff√©rentes M√©triques')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Mod√®le')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/model_comparison_metrics.png')\n",
    "plt.close()\n",
    "logging.info('Figure de comparaison des m√©triques enregistr√©e dans figures/model_comparison_metrics.png')\n",
    "\n",
    "# Figure pour le score final\n",
    "benchmark_df['final_score'].plot(kind='bar', figsize=(8, 6))\n",
    "plt.title('Score Final Pond√©r√© des Mod√®les')\n",
    "plt.ylabel('Score Final')\n",
    "plt.xlabel('Mod√®le')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/model_final_scores.png')\n",
    "plt.close()\n",
    "logging.info('Figure des scores finaux enregistr√©e dans figures/model_final_scores.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Conclusion et Prochaines √âtapes\n",
    "\n",
    "### ‚úÖ R√©alisations\n",
    "- Pipeline ML complet de bout en bout\n",
    "- 3 mod√®les entra√Æn√©s et optimis√©s\n",
    "- √âvaluation rigoureuse sur ensemble de validation\n",
    "- S√©lection objective du meilleur mod√®le\n",
    "- Artefacts sauvegard√©s pour d√©ploiement\n",
    "\n",
    "### üîÑ Prochaines √âtapes\n",
    "\n",
    "**1. Validation finale**\n",
    "- √âvaluer le mod√®le s√©lectionn√© sur le test set (15% des donn√©es non vues)\n",
    "- V√©rifier qu'il n'y a pas d'overfitting\n",
    "\n",
    "**2. Analyse d'erreurs**\n",
    "- Identifier les patterns de faux n√©gatifs (clients churned non d√©tect√©s)\n",
    "- Analyser les caract√©ristiques des faux positifs\n",
    "\n",
    "**3. Feature importance**\n",
    "- Pour Random Forest : analyser l'importance des features\n",
    "- Pour Logistic Regression : interpr√©ter les coefficients\n",
    "\n",
    "**4. Am√©lioration du mod√®le**\n",
    "- Feature engineering avanc√©\n",
    "- Gestion du d√©s√©quilibre des classes (SMOTE, class weights)\n",
    "- Test d'algorithmes additionnels (XGBoost, LightGBM)\n",
    "\n",
    "**5. D√©ploiement**\n",
    "- Cr√©er une API REST avec le mod√®le\n",
    "- Int√©grer dans l'application Streamlit (d√©j√† fait)\n",
    "- Mise en place du monitoring en production\n",
    "\n",
    "### üìö Ressources\n",
    "- Documentation compl√®te dans le dossier `docs/`\n",
    "- Guides de bonnes pratiques ML et d√©ploiement\n",
    "- Application Streamlit pour l'inf√©rence interactive\n",
    "\n",
    "---\n",
    "**üìß Questions ou suggestions ?** Consultez le README.md ou ouvrez une issue sur GitHub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
