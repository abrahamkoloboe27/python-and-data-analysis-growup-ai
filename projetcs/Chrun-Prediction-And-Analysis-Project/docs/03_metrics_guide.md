# üìä Guide Complet des M√©triques en Machine Learning

## Table des Mati√®res
- [Introduction](#introduction)
- [M√©triques de R√©gression](#m√©triques-de-r√©gression)
- [M√©triques de Classification](#m√©triques-de-classification)
- [M√©triques de Clustering](#m√©triques-de-clustering)
- [Comment Choisir ses M√©triques](#comment-choisir-ses-m√©triques)
- [Combiner Plusieurs M√©triques](#combiner-plusieurs-m√©triques)
- [M√©triques Business vs M√©triques ML](#m√©triques-business-vs-m√©triques-ml)

## Introduction

Les m√©triques sont essentielles pour √©valuer la performance des mod√®les ML. Choisir les bonnes m√©triques d√©pend :
- Du **type de probl√®me** (r√©gression, classification, etc.)
- Du **contexte business**
- Des **co√ªts** associ√©s aux erreurs
- De la **distribution** des classes

> ‚ö†Ô∏è **R√®gle d'Or** : Ne jamais se fier √† une seule m√©trique. Toujours en combiner plusieurs pour une √©valuation compl√®te.

---

## M√©triques de R√©gression

### 1. MAE (Mean Absolute Error)

**Formule** :
```
MAE = (1/n) √ó Œ£|y·µ¢ - ≈∑·µ¢|
```

**Caract√©ristiques** :
- ‚úÖ Interpr√©table (m√™me unit√© que y)
- ‚úÖ Robuste aux outliers
- ‚ùå Ne p√©nalise pas fortement les grandes erreurs

**Quand l'utiliser** :
- Quand les outliers ne doivent pas avoir trop d'impact
- Quand on veut une m√©trique facile √† expliquer
- Pour des pr√©dictions de prix, distances

**Exemple** :
```python
from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(y_true, y_pred)
print(f"MAE: {mae:.2f} euros")  # Ex: "MAE: 50.00 euros"
```

---

### 2. MSE (Mean Squared Error)

**Formule** :
```
MSE = (1/n) √ó Œ£(y·µ¢ - ≈∑·µ¢)¬≤
```

**Caract√©ristiques** :
- ‚úÖ P√©nalise fortement les grandes erreurs
- ‚ùå Pas la m√™me unit√© que y (unit√©¬≤)
- ‚ùå Sensible aux outliers

**Quand l'utiliser** :
- Quand les grandes erreurs sont tr√®s co√ªteuses
- Pour l'optimisation (d√©rivable)
- En interne pour l'entra√Ænement

**Exemple** :
```python
from sklearn.metrics import mean_squared_error

mse = mean_squared_error(y_true, y_pred)
print(f"MSE: {mse:.2f}")
```

---

### 3. RMSE (Root Mean Squared Error)

**Formule** :
```
RMSE = ‚àöMSE = ‚àö[(1/n) √ó Œ£(y·µ¢ - ≈∑·µ¢)¬≤]
```

**Caract√©ristiques** :
- ‚úÖ M√™me unit√© que y
- ‚úÖ P√©nalise fortement les grandes erreurs
- ‚ùå Sensible aux outliers

**Quand l'utiliser** :
- Version "interpr√©table" du MSE
- M√©trique standard pour comparer des mod√®les
- Quand on veut p√©naliser les grandes erreurs

**Exemple** :
```python
import numpy as np
from sklearn.metrics import mean_squared_error

rmse = np.sqrt(mean_squared_error(y_true, y_pred))
print(f"RMSE: {rmse:.2f} euros")
```

---

### 4. R¬≤ (Coefficient de D√©termination)

**Formule** :
```
R¬≤ = 1 - (SS_res / SS_tot)
o√π SS_res = Œ£(y·µ¢ - ≈∑·µ¢)¬≤
et SS_tot = Œ£(y·µ¢ - »≥)¬≤
```

**Caract√©ristiques** :
- ‚úÖ Sans unit√© (entre -‚àû et 1)
- ‚úÖ Facile √† interpr√©ter
- ‚ùå Peut √™tre trompeur avec des mod√®les complexes

**Interpr√©tation** :
- R¬≤ = 1 : Mod√®le parfait
- R¬≤ = 0 : Mod√®le √©quivalent √† la moyenne
- R¬≤ < 0 : Mod√®le pire que la moyenne

**Quand l'utiliser** :
- Pour expliquer la variance captur√©e
- Comparer des mod√®les sur le m√™me dataset
- Communication avec non-experts

**Exemple** :
```python
from sklearn.metrics import r2_score

r2 = r2_score(y_true, y_pred)
print(f"R¬≤: {r2:.3f}")  # Ex: "R¬≤: 0.856" ‚Üí 85.6% de variance expliqu√©e
```

---

### 5. MAPE (Mean Absolute Percentage Error)

**Formule** :
```
MAPE = (100/n) √ó Œ£|((y·µ¢ - ≈∑·µ¢) / y·µ¢)|
```

**Caract√©ristiques** :
- ‚úÖ Ind√©pendante de l'√©chelle
- ‚úÖ Facile √† interpr√©ter (%)
- ‚ùå Probl√®me si y·µ¢ = 0
- ‚ùå Asym√©trique

**Quand l'utiliser** :
- Pour comparer des mod√®les sur diff√©rents datasets
- Quand l'erreur relative est importante
- S√©ries temporelles

**Exemple** :
```python
def mape(y_true, y_pred):
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

mape_score = mape(y_true, y_pred)
print(f"MAPE: {mape_score:.2f}%")  # Ex: "MAPE: 5.23%"
```

---

### Tableau Comparatif : M√©triques de R√©gression

| M√©trique | Interpr√©tabilit√© | Robuste aux Outliers | Quand privil√©gier |
|----------|------------------|----------------------|-------------------|
| **MAE** | ‚≠ê‚≠ê‚≠ê | ‚úÖ | Outliers pr√©sents, interpr√©tation simple |
| **MSE** | ‚≠ê | ‚ùå | Optimisation, grandes erreurs co√ªteuses |
| **RMSE** | ‚≠ê‚≠ê‚≠ê | ‚ùå | Standard, grandes erreurs importantes |
| **R¬≤** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | Variance expliqu√©e, communication |
| **MAPE** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | Erreur relative, comparaison multi-datasets |

---

## M√©triques de Classification

### Matrice de Confusion

La base de toutes les m√©triques de classification :

```
                Pr√©diction
                Positive    Negative
R√©alit√©  Pos    TP          FN
         Neg    FP          TN
```

- **TP** (True Positive) : Correctement pr√©dit positif
- **TN** (True Negative) : Correctement pr√©dit n√©gatif
- **FP** (False Positive) : Incorrectement pr√©dit positif (Erreur Type I)
- **FN** (False Negative) : Incorrectement pr√©dit n√©gatif (Erreur Type II)

---

### 1. Accuracy (Exactitude)

**Formule** :
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

**Caract√©ristiques** :
- ‚úÖ Simple, intuitif
- ‚ùå Trompeur avec classes d√©s√©quilibr√©es
- ‚ùå Ne diff√©rencie pas les types d'erreurs

**Quand l'utiliser** :
- Classes √©quilibr√©es
- Co√ªts d'erreur similaires
- Baseline simple

**‚ö†Ô∏è Exemple du Pi√®ge** :
```
Dataset : 95% classe 0, 5% classe 1
Mod√®le na√Øf pr√©disant toujours 0 ‚Üí Accuracy = 95% !
```

**Exemple** :
```python
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_true, y_pred)
print(f"Accuracy: {accuracy:.3f}")  # Ex: 0.856
```

---

### 2. Precision (Pr√©cision)

**Formule** :
```
Precision = TP / (TP + FP)
```

**Signification** : Parmi les pr√©dictions positives, combien sont vraiment positives ?

**Quand l'utiliser** :
- Le co√ªt d'un **False Positive est √©lev√©**
- Spam detection (√©viter de marquer un vrai email comme spam)
- Recommandations (√©viter de recommander des mauvais items)

**Exemple** :
```python
from sklearn.metrics import precision_score

precision = precision_score(y_true, y_pred)
print(f"Precision: {precision:.3f}")
```

---

### 3. Recall (Rappel / Sensibilit√©)

**Formule** :
```
Recall = TP / (TP + FN)
```

**Signification** : Parmi les vraies positifs, combien sont d√©tect√©s ?

**Quand l'utiliser** :
- Le co√ªt d'un **False Negative est √©lev√©**
- D√©tection de fraude (ne pas manquer une fraude)
- Diagnostic m√©dical (ne pas manquer une maladie)
- D√©tection de churn (ne pas manquer un client √† risque)

**Exemple** :
```python
from sklearn.metrics import recall_score

recall = recall_score(y_true, y_pred)
print(f"Recall: {recall:.3f}")
```

---

### 4. F1-Score

**Formule** :
```
F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)
```

**Caract√©ristiques** :
- ‚úÖ √âquilibre entre Precision et Recall
- ‚úÖ Bon pour classes d√©s√©quilibr√©es
- ‚ùå Traite Precision et Recall √©galement

**Quand l'utiliser** :
- Classes d√©s√©quilibr√©es
- Besoin d'√©quilibre entre Precision et Recall
- M√©trique unique pour comparer des mod√®les

**Variantes** :
- **F2-Score** : Favorise le Recall (Œ≤=2)
- **F0.5-Score** : Favorise la Precision (Œ≤=0.5)

**Exemple** :
```python
from sklearn.metrics import f1_score

f1 = f1_score(y_true, y_pred)
print(f"F1-Score: {f1:.3f}")
```

---

### 5. ROC-AUC (Area Under the ROC Curve)

**Description** :
- Courbe ROC : True Positive Rate vs False Positive Rate
- AUC : Aire sous cette courbe

**Caract√©ristiques** :
- ‚úÖ Ind√©pendant du seuil de d√©cision
- ‚úÖ Bon pour classes d√©s√©quilibr√©es
- ‚úÖ Compare la capacit√© de discrimination

**Interpr√©tation** :
- AUC = 1.0 : Mod√®le parfait
- AUC = 0.5 : Mod√®le al√©atoire
- AUC < 0.5 : Pire qu'al√©atoire (inversez les pr√©dictions!)

**Quand l'utiliser** :
- Comparer des mod√®les
- Quand le seuil n'est pas fix√©
- Classes d√©s√©quilibr√©es

**Exemple** :
```python
from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# Calcul AUC
auc = roc_auc_score(y_true, y_pred_proba)
print(f"ROC-AUC: {auc:.3f}")

# Courbe ROC
fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)
plt.plot(fpr, tpr, label=f'AUC = {auc:.3f}')
plt.plot([0, 1], [0, 1], 'k--', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()
```

---

### 6. Precision-Recall AUC

**Description** :
- Alternative √† ROC-AUC
- Courbe Precision vs Recall

**Quand l'utiliser** :
- **Classes tr√®s d√©s√©quilibr√©es**
- Classe positive rare et importante
- Meilleure que ROC-AUC dans ces cas

**Exemple** :
```python
from sklearn.metrics import precision_recall_curve, auc

precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)
pr_auc = auc(recall, precision)
print(f"PR-AUC: {pr_auc:.3f}")
```

---

### 7. Log Loss (Cross-Entropy)

**Formule** :
```
Log Loss = -(1/n) √ó Œ£[y·µ¢√ólog(≈∑·µ¢) + (1-y·µ¢)√ólog(1-≈∑·µ¢)]
```

**Caract√©ristiques** :
- ‚úÖ Prend en compte les probabilit√©s (pas juste 0/1)
- ‚úÖ P√©nalise les pr√©dictions confiantes mais fausses
- ‚ùå Moins interpr√©table

**Quand l'utiliser** :
- Probabilit√©s calibr√©es importantes
- Optimisation de mod√®les
- Comp√©titions Kaggle

**Exemple** :
```python
from sklearn.metrics import log_loss

logloss = log_loss(y_true, y_pred_proba)
print(f"Log Loss: {logloss:.3f}")  # Plus bas = meilleur
```

---

### Choisir entre Precision et Recall

| Contexte | Privil√©gier | Raison |
|----------|-------------|--------|
| **Spam Detection** | Precision | √âviter de bloquer vrais emails |
| **Fraud Detection** | Recall | Ne pas manquer une fraude |
| **Medical Diagnosis (cancer)** | Recall | Ne pas manquer un malade |
| **Recommended Products** | Precision | √âviter mauvaises recommandations |
| **Churn Prediction** | Recall | Ne pas manquer clients √† risque |
| **Content Moderation** | Recall | Ne pas manquer contenu inappropri√© |

---

### Tableau Comparatif : M√©triques de Classification

| M√©trique | Classes D√©s√©quilibr√©es | Interpr√©tabilit√© | Usage Principal |
|----------|------------------------|------------------|-----------------|
| **Accuracy** | ‚ùå | ‚≠ê‚≠ê‚≠ê | Baseline, classes √©quilibr√©es |
| **Precision** | ‚úÖ | ‚≠ê‚≠ê‚≠ê | FP co√ªteux |
| **Recall** | ‚úÖ | ‚≠ê‚≠ê‚≠ê | FN co√ªteux |
| **F1-Score** | ‚úÖ | ‚≠ê‚≠ê | √âquilibre Precision-Recall |
| **ROC-AUC** | ‚úÖ | ‚≠ê‚≠ê | Comparaison mod√®les |
| **PR-AUC** | ‚úÖ‚úÖ | ‚≠ê‚≠ê | Classes tr√®s d√©s√©quilibr√©es |
| **Log Loss** | ‚úÖ | ‚≠ê | Probabilit√©s calibr√©es |

---

## M√©triques de Clustering

### 1. Silhouette Score

**Formule** :
```
s(i) = (b(i) - a(i)) / max(a(i), b(i))
o√π a(i) = distance moyenne intra-cluster
et b(i) = distance moyenne au cluster le plus proche
```

**Interpr√©tation** :
- Score entre -1 et 1
- Proche de 1 : Bon clustering
- Proche de 0 : Sur la fronti√®re
- N√©gatif : Probablement dans le mauvais cluster

**Exemple** :
```python
from sklearn.metrics import silhouette_score

score = silhouette_score(X, cluster_labels)
print(f"Silhouette Score: {score:.3f}")
```

---

### 2. Davies-Bouldin Index

**Caract√©ristiques** :
- Plus bas = meilleur
- Mesure le ratio dispersion/s√©paration

**Exemple** :
```python
from sklearn.metrics import davies_bouldin_score

db_score = davies_bouldin_score(X, cluster_labels)
print(f"Davies-Bouldin: {db_score:.3f}")  # Plus bas = meilleur
```

---

### 3. Calinski-Harabasz Index

**Caract√©ristiques** :
- Plus √©lev√© = meilleur
- Variance inter-cluster vs intra-cluster

**Exemple** :
```python
from sklearn.metrics import calinski_harabasz_score

ch_score = calinski_harabasz_score(X, cluster_labels)
print(f"Calinski-Harabasz: {ch_score:.2f}")  # Plus √©lev√© = meilleur
```

---

## Comment Choisir ses M√©triques

### √âtape 1 : Identifier le Type de Probl√®me

```
R√©gression ‚Üí MAE, RMSE, R¬≤
Classification ‚Üí Accuracy, F1, ROC-AUC
Clustering ‚Üí Silhouette, Davies-Bouldin
```

### √âtape 2 : Analyser le Contexte Business

**Questions √† se poser** :
1. Les classes sont-elles √©quilibr√©es ?
2. Quel type d'erreur est le plus co√ªteux ?
3. Ai-je besoin de probabilit√©s calibr√©es ?
4. Dois-je expliquer √† des non-experts ?

### √âtape 3 : Consid√©rer les Contraintes

- **Interpr√©tabilit√©** : Privil√©gier MAE, Accuracy, Precision, Recall
- **Performance pure** : ROC-AUC, RMSE, Log Loss
- **Classes d√©s√©quilibr√©es** : F1, ROC-AUC, PR-AUC
- **Communication** : R¬≤, Accuracy, MAPE

---

## Combiner Plusieurs M√©triques

### Principe Fondamental

> ‚ö†Ô∏è **Ne JAMAIS se fier √† une seule m√©trique !**

### Approches de Combinaison

#### 1. Ensemble Compl√©mentaire
```python
# Classification d√©s√©quilibr√©e
metrics = {
    'accuracy': accuracy_score(y_true, y_pred),
    'precision': precision_score(y_true, y_pred),
    'recall': recall_score(y_true, y_pred),
    'f1': f1_score(y_true, y_pred),
    'roc_auc': roc_auc_score(y_true, y_pred_proba)
}
```

#### 2. M√©trique Principale + M√©triques Secondaires

**Exemple Churn Prediction** :
- **Principale** : Recall (ne pas manquer clients √† risque)
- **Secondaires** : Precision (√©viter trop de faux positifs), F1, ROC-AUC

#### 3. Seuil de Performance Multi-M√©triques

```python
# Un mod√®le est acceptable si :
acceptable = (
    recall >= 0.75 and      # Capture 75% des churns
    precision >= 0.60 and   # 60% de vrais positifs
    f1 >= 0.65             # Bon √©quilibre g√©n√©ral
)
```

---

### Strat√©gies par Type de Probl√®me

#### Classification Binaire D√©s√©quilibr√©e (ex: Churn)
```python
primary_metrics = ['recall', 'f1_score']
secondary_metrics = ['precision', 'roc_auc']
monitoring_metrics = ['confusion_matrix', 'classification_report']
```

#### R√©gression (ex: Prix)
```python
primary_metrics = ['rmse', 'mae']
secondary_metrics = ['r2', 'mape']
visual_metrics = ['residual_plots', 'prediction_vs_actual']
```

#### Multi-classe
```python
primary_metrics = ['macro_f1', 'weighted_f1']
secondary_metrics = ['accuracy', 'per_class_recall']
monitoring_metrics = ['confusion_matrix']
```

---

## M√©triques Business vs M√©triques ML

### Diff√©rence Fondamentale

| Aspect | M√©triques ML | M√©triques Business |
|--------|--------------|-------------------|
| **Focus** | Performance du mod√®le | Impact business |
| **Exemples** | Accuracy, F1, RMSE | ROI, Revenue, Cost savings |
| **Audience** | Data Scientists | Stakeholders, Management |
| **Temporalit√©** | Imm√©diate | Long terme |

### Relier ML et Business

#### Exemple : Churn Prediction

**M√©triques ML** :
- Recall = 0.75
- Precision = 0.60
- F1 = 0.67

**Traduction Business** :
- Co√ªt moyen d'acquisition client : 500‚Ç¨
- Valeur vie client (CLV) : 2000‚Ç¨
- Co√ªt campagne r√©tention : 50‚Ç¨

**Calcul ROI** :
```python
# Sur 1000 clients √† risque d√©tect√©s
TP = 750   # Recall = 0.75
FP = 500   # Precision = 0.60

# Avec intervention
clients_saved = TP * 0.50  # 50% sauv√©s = 375 clients
revenue_saved = clients_saved * 2000  # 750,000‚Ç¨

# Co√ªts
campaign_cost = 1250 * 50  # 62,500‚Ç¨

# ROI
roi = (revenue_saved - campaign_cost) / campaign_cost
# ROI = 11x ‚Üí Excellent !
```

### Dashboard de M√©triques Compl√®te

```python
# 1. M√©triques ML (pour l'√©quipe DS)
ml_metrics = {
    'accuracy': 0.85,
    'precision': 0.60,
    'recall': 0.75,
    'f1': 0.67,
    'roc_auc': 0.82
}

# 2. M√©triques Business (pour stakeholders)
business_metrics = {
    'clients_saved': 375,
    'revenue_saved': '750K‚Ç¨',
    'campaign_cost': '62.5K‚Ç¨',
    'roi': '11x',
    'cost_per_save': '167‚Ç¨'
}

# 3. M√©triques Op√©rationnelles (pour la production)
operational_metrics = {
    'prediction_latency': '50ms',
    'model_uptime': '99.9%',
    'data_quality_score': 0.95,
    'predictions_per_day': 10000
}
```

---

## Guide de D√©cision Rapide

### Pour Classification

```python
# Classes √©quilibr√©es + erreurs similaires
‚Üí Accuracy + Confusion Matrix

# Classes d√©s√©quilibr√©es + FN co√ªteux (ex: churn, fraude)
‚Üí Recall + F1 + ROC-AUC

# Classes d√©s√©quilibr√©es + FP co√ªteux (ex: spam)
‚Üí Precision + F1 + PR-AUC

# Probabilit√©s importantes
‚Üí Log Loss + ROC-AUC

# Comparaison de mod√®les
‚Üí ROC-AUC (ou PR-AUC si tr√®s d√©s√©quilibr√©)
```

### Pour R√©gression

```python
# Standard, interpr√©tation
‚Üí RMSE + R¬≤ + MAE

# Outliers pr√©sents
‚Üí MAE + R¬≤ + Median Absolute Error

# Erreur relative importante
‚Üí MAPE + RMSE

# Comparaison multi-datasets
‚Üí MAPE + R¬≤
```

---

## Bonnes Pratiques

### ‚úÖ √Ä Faire

1. **Toujours** utiliser plusieurs m√©triques compl√©mentaires
2. **Aligner** m√©triques ML avec objectifs business
3. **Documenter** le choix des m√©triques et pourquoi
4. **Visualiser** : Confusion matrix, courbes ROC/PR, residual plots
5. **Monitorer** en production : drift des m√©triques
6. **Communiquer** diff√©remment selon l'audience

### ‚ùå √Ä √âviter

1. ‚ùå Se fier uniquement √† l'accuracy sur classes d√©s√©quilibr√©es
2. ‚ùå Optimiser une m√©trique sans consid√©rer le business
3. ‚ùå Ignorer les distributions de classes
4. ‚ùå Utiliser m√©triques de train pour comparer mod√®les (utiliser validation/test)
5. ‚ùå Oublier de calibrer les probabilit√©s si n√©cessaires

---

## Exemple Complet : Pipeline d'√âvaluation

```python
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, classification_report, confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns

def evaluate_classifier(y_true, y_pred, y_pred_proba):
    """√âvaluation compl√®te d'un classificateur"""
    
    # 1. M√©triques de base
    metrics = {
        'Accuracy': accuracy_score(y_true, y_pred),
        'Precision': precision_score(y_true, y_pred),
        'Recall': recall_score(y_true, y_pred),
        'F1-Score': f1_score(y_true, y_pred),
        'ROC-AUC': roc_auc_score(y_true, y_pred_proba)
    }
    
    # 2. Afficher les m√©triques
    print("=== M√âTRIQUES DE PERFORMANCE ===")
    for name, value in metrics.items():
        print(f"{name}: {value:.3f}")
    
    # 3. Classification report
    print("\n=== CLASSIFICATION REPORT ===")
    print(classification_report(y_true, y_pred))
    
    # 4. Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()
    
    # 5. ROC Curve
    from sklearn.metrics import roc_curve
    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {metrics["ROC-AUC"]:.3f})')
    plt.plot([0, 1], [0, 1], 'k--', label='Random')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend()
    plt.show()
    
    return metrics

# Utilisation
metrics = evaluate_classifier(y_test, y_pred, y_pred_proba[:, 1])
```

---

## Ressources Compl√©mentaires

### Documentation
- [Scikit-learn Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)
- [Classification Metrics Explained](https://towardsdatascience.com/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd)

### Outils
- **MLflow** : Tracking des m√©triques
- **Weights & Biases** : Monitoring et visualisation
- **TensorBoard** : Pour Deep Learning

---

**Navigation**
- [‚Üê Pr√©c√©dent : Types de T√¢ches ML](02_ml_task_types.md)
- [Suivant : Guide Churn Prediction ‚Üí](04_churn_prediction_guide.md)
- [Retour au README principal](../README.md)
