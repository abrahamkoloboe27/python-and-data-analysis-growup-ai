# üìã Types de T√¢ches en Machine Learning

## Table des Mati√®res
- [Introduction](#introduction)
- [1. R√©gression (Regression)](#1-r√©gression-regression)
- [2. Classification](#2-classification)
- [3. Clustering](#3-clustering)
- [4. D√©tection d'Anomalies (Anomaly Detection)](#4-d√©tection-danomalies-anomaly-detection)
- [5. S√©ries Temporelles (Time Series)](#5-s√©ries-temporelles-time-series)
- [6. Autres Types de T√¢ches](#6-autres-types-de-t√¢ches)
- [Comment Choisir ?](#comment-choisir)

## Introduction

Le Machine Learning couvre diff√©rents types de t√¢ches, chacune adapt√©e √† des probl√®mes sp√©cifiques. Ce guide d√©taille les principales cat√©gories de t√¢ches ML, leurs caract√©ristiques, algorithmes et cas d'usage.

## 1. R√©gression (Regression)

### Description
Pr√©dire une **valeur num√©rique continue** √† partir de features.

### Caract√©ristiques
- **Variable cible** : Continue (nombres r√©els)
- **Type d'apprentissage** : Supervis√©
- **Output** : Un nombre (ex: 45.6, 1200.5)

### Algorithmes Courants
| Algorithme | Avantages | Inconv√©nients | Quand l'utiliser |
|------------|-----------|---------------|------------------|
| **R√©gression Lin√©aire** | Simple, interpr√©table | Assume lin√©arit√© | Relations lin√©aires |
| **Ridge/Lasso** | R√©gularisation | Tuning des hyperparam√®tres | Beaucoup de features |
| **Random Forest Regressor** | G√®re non-lin√©arit√© | Moins interpr√©table | Relations complexes |
| **XGBoost Regressor** | Tr√®s performant | Temps d'entra√Ænement | Comp√©titions, production |
| **SVR** | Efficace en haute dimension | Co√ªteux en calcul | Datasets petits/moyens |
| **Neural Networks** | Tr√®s flexible | N√©cessite beaucoup de donn√©es | Donn√©es massives |

### Cas d'Usage
- üí∞ **Pr√©diction de prix** : immobilier, actions
- üìà **Pr√©visions de ventes**
- üå°Ô∏è **Pr√©diction de temp√©rature**
- ‚è±Ô∏è **Estimation de temps de livraison**
- üíµ **Pr√©diction de revenus**

### Exemple de Code
```python
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# R√©gression lin√©aire
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
y_pred = lr_model.predict(X_test)

# Random Forest
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# √âvaluation
print(f"MSE: {mean_squared_error(y_test, y_pred)}")
print(f"R¬≤: {r2_score(y_test, y_pred)}")
```

### M√©triques Principales
- **MSE** (Mean Squared Error) : P√©nalise fortement les grandes erreurs
- **RMSE** (Root Mean Squared Error) : M√™me unit√© que la variable cible
- **MAE** (Mean Absolute Error) : Moins sensible aux outliers
- **R¬≤** (Coefficient de d√©termination) : Proportion de variance expliqu√©e

---

## 2. Classification

### Description
Pr√©dire une **cat√©gorie** ou **classe** discr√®te √† partir de features.

### 2.1 Classification Binaire
- **Variable cible** : 2 classes (0/1, Oui/Non, True/False)
- **Exemples** : Spam/Not Spam, Fraud/Not Fraud, Churn/No Churn

### 2.2 Classification Multi-classe
- **Variable cible** : > 2 classes mutuellement exclusives
- **Exemples** : Reconnaissance de chiffres (0-9), Cat√©gories de produits

### 2.3 Classification Multi-label
- **Variable cible** : Plusieurs labels simultan√©s
- **Exemples** : Tags d'articles, Genres de films

### Algorithmes Courants
| Algorithme | Avantages | Inconv√©nients | Quand l'utiliser |
|------------|-----------|---------------|------------------|
| **R√©gression Logistique** | Simple, rapide, interpr√©table | Assume lin√©arit√© | Baseline, probl√®mes lin√©aires |
| **Decision Trees** | Interpr√©table | Overfitting facile | R√®gles m√©tier claires |
| **Random Forest** | Robuste, performant | Bo√Æte noire | Production, bonnes perfs |
| **XGBoost/LightGBM** | Excellentes performances | Hyperparam√®tres complexes | Comp√©titions |
| **SVM** | Efficace en haute dimension | Pas scalable | Petits datasets |
| **Neural Networks** | Tr√®s flexible | Beaucoup de donn√©es n√©cessaires | Images, texte, donn√©es complexes |
| **KNN** | Simple | Lent en pr√©diction | Petits datasets |
| **Naive Bayes** | Rapide, efficace | Assume ind√©pendance | Classification de texte |

### Cas d'Usage
- üìß **D√©tection de spam**
- üè• **Diagnostic m√©dical**
- üí≥ **D√©tection de fraude**
- üë§ **Pr√©diction de churn** (notre projet!)
- üñºÔ∏è **Classification d'images**
- üìù **Analyse de sentiment**
- üîç **Reconnaissance de caract√®res**

### Exemple de Code
```python
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

# R√©gression Logistique
lr_model = LogisticRegression(random_state=42)
lr_model.fit(X_train, y_train)
y_pred = lr_model.predict(X_test)
y_pred_proba = lr_model.predict_proba(X_test)[:, 1]

# Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# √âvaluation
print(classification_report(y_test, y_pred))
print(f"ROC-AUC: {roc_auc_score(y_test, y_pred_proba)}")
```

### M√©triques Principales
- **Accuracy** : % de pr√©dictions correctes
- **Precision** : TP / (TP + FP)
- **Recall** : TP / (TP + FN)
- **F1-Score** : Moyenne harmonique de Precision et Recall
- **ROC-AUC** : Aire sous la courbe ROC
- **Confusion Matrix** : Matrice de confusion

---

## 3. Clustering

### Description
Regrouper des donn√©es similaires sans labels pr√©existants (**Apprentissage non supervis√©**).

### Caract√©ristiques
- **Type d'apprentissage** : Non supervis√©
- **Objectif** : D√©couvrir des groupes naturels
- **Pas de variable cible**

### Algorithmes Courants
| Algorithme | Avantages | Inconv√©nients | Quand l'utiliser |
|------------|-----------|---------------|------------------|
| **K-Means** | Rapide, simple | Nombre de clusters √† d√©finir | Clusters sph√©riques |
| **DBSCAN** | Trouve clusters de forme arbitraire | Sensible aux param√®tres | D√©tection d'outliers |
| **Hierarchical Clustering** | Pas besoin de K | Pas scalable | Petit dataset, dendrogrammes |
| **Gaussian Mixture Models** | Soft clustering | Plus complexe | Clusters qui se chevauchent |

### Cas d'Usage
- üë• **Segmentation de clients**
- üó∫Ô∏è **Analyse g√©ographique**
- üß¨ **Analyse g√©n√©tique**
- üì∞ **Groupement de documents**
- üé® **Compression d'images**

### Exemple de Code
```python
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score

# K-Means
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(X)

# √âvaluation
score = silhouette_score(X, clusters)
print(f"Silhouette Score: {score}")

# DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
clusters_db = dbscan.fit_predict(X)
```

### M√©triques Principales
- **Silhouette Score** : Coh√©sion et s√©paration des clusters
- **Inertia** : Somme des distances au centre (K-Means)
- **Davies-Bouldin Index** : Ratio dispersion/s√©paration
- **Calinski-Harabasz Index** : Ratio variance inter/intra cluster

---

## 4. D√©tection d'Anomalies (Anomaly Detection)

### Description
Identifier les observations qui d√©vient significativement du comportement normal.

### Caract√©ristiques
- **Type d'apprentissage** : G√©n√©ralement non supervis√© ou semi-supervis√©
- **Objectif** : Trouver les points inhabituels
- **Classes d√©s√©quilibr√©es** : Anomalies rares

### Algorithmes Courants
| Algorithme | Avantages | Inconv√©nients | Quand l'utiliser |
|------------|-----------|---------------|------------------|
| **Isolation Forest** | Efficace, rapide | Param√®tres √† tuner | D√©tection g√©n√©rale |
| **One-Class SVM** | Robuste | Pas scalable | Datasets petits |
| **Local Outlier Factor** | D√©tecte anomalies locales | Co√ªteux en calcul | Anomalies locales |
| **Autoencoders** | Capture patterns complexes | N√©cessite beaucoup de donn√©es | Donn√©es complexes |
| **Statistical Methods** | Simple, interpr√©table | Assume distribution | Donn√©es simples |

### Cas d'Usage
- üí≥ **D√©tection de fraude bancaire**
- üè≠ **Maintenance pr√©dictive** (d√©faillances machines)
- üîê **Cybers√©curit√©** (intrusions r√©seau)
- üè• **Diagnostic m√©dical** (cas rares)
- üìä **Contr√¥le qualit√©**

### Exemple de Code
```python
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor

# Isolation Forest
iso_forest = IsolationForest(contamination=0.1, random_state=42)
anomalies = iso_forest.fit_predict(X)

# LOF
lof = LocalOutlierFactor(contamination=0.1)
anomalies_lof = lof.fit_predict(X)

# -1 = anomalie, 1 = normal
print(f"Anomalies d√©tect√©es: {(anomalies == -1).sum()}")
```

### M√©triques Principales
- **Precision/Recall** : Si labels disponibles
- **F1-Score** : √âquilibre entre pr√©cision et rappel
- **Contamination Rate** : Proportion d'anomalies attendues

---

## 5. S√©ries Temporelles (Time Series)

### Description
Analyser et pr√©dire des donn√©es s√©quentielles d√©pendant du temps.

### Caract√©ristiques
- **D√©pendance temporelle** : L'ordre des observations est crucial
- **Composantes** : Tendance, saisonnalit√©, cyclicit√©, bruit
- **Types** : Univari√© (une variable) ou Multivari√© (plusieurs variables)

### Approches Principales

#### 5.1 M√©thodes Statistiques
- **ARIMA** : AutoRegressive Integrated Moving Average
- **SARIMA** : ARIMA avec saisonnalit√©
- **Prophet** : D√©velopp√© par Facebook
- **Exponential Smoothing**

#### 5.2 Machine Learning
- **Regression Models** : Avec features temporelles
- **Random Forest/XGBoost** : Pour s√©ries complexes

#### 5.3 Deep Learning
- **LSTM** : Long Short-Term Memory
- **GRU** : Gated Recurrent Unit
- **Transformer** : Architecture attention

### Cas d'Usage
- üìà **Pr√©vision de ventes**
- üíπ **Pr√©diction de cours boursiers**
- üå§Ô∏è **Pr√©visions m√©t√©orologiques**
- ‚ö° **Pr√©diction de consommation √©nerg√©tique**
- üö¶ **Pr√©diction de trafic**

### Exemple de Code
```python
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_absolute_error

# ARIMA
model = ARIMA(train_data, order=(1, 1, 1))
model_fit = model.fit()
forecast = model_fit.forecast(steps=10)

# Avec sklearn (features temporelles)
from sklearn.ensemble import RandomForestRegressor

# Cr√©er des features temporelles
df['lag_1'] = df['value'].shift(1)
df['lag_7'] = df['value'].shift(7)
df['rolling_mean_7'] = df['value'].rolling(7).mean()

# Entra√Æner le mod√®le
model = RandomForestRegressor()
model.fit(X_train, y_train)
```

### M√©triques Principales
- **MAE** : Mean Absolute Error
- **RMSE** : Root Mean Squared Error
- **MAPE** : Mean Absolute Percentage Error
- **SMAPE** : Symmetric MAPE

---

## 6. Autres Types de T√¢ches

### 6.1 R√©duction de Dimensionnalit√©
**Objectif** : R√©duire le nombre de features tout en conservant l'information

**Algorithmes** :
- **PCA** (Principal Component Analysis)
- **t-SNE** (pour visualisation)
- **UMAP** (pour visualisation)
- **Autoencoders**

**Use Cases** :
- Visualisation de donn√©es haute dimension
- R√©duction de bruit
- Feature extraction

### 6.2 Ranking
**Objectif** : Ordonner des items selon leur pertinence

**Use Cases** :
- Moteurs de recherche
- Syst√®mes de recommandation
- Publicit√© en ligne

### 6.3 Recommandation
**Objectif** : Sugg√©rer des items pertinents √† un utilisateur

**Approches** :
- **Collaborative Filtering** : Bas√© sur comportements similaires
- **Content-Based** : Bas√© sur caract√©ristiques des items
- **Hybrid** : Combinaison des deux

**Use Cases** :
- Netflix, YouTube (recommandations de vid√©os)
- E-commerce (produits sugg√©r√©s)
- Spotify (musique)

### 6.4 NLP (Natural Language Processing)
**T√¢ches** :
- Classification de texte
- Named Entity Recognition (NER)
- Machine Translation
- Question Answering
- Summarization

**Algorithmes** :
- BERT, GPT, T5 (Transformers)
- RNN, LSTM
- Bag of Words, TF-IDF

### 6.5 Computer Vision
**T√¢ches** :
- Classification d'images
- D√©tection d'objets
- Segmentation
- Face Recognition

**Algorithmes** :
- CNN (Convolutional Neural Networks)
- ResNet, VGG, EfficientNet
- YOLO, R-CNN (d√©tection)

---

## Comment Choisir ?

### Diagramme de D√©cision

```
Avez-vous des labels (variable cible) ?
‚îÇ
‚îú‚îÄ OUI ‚Üí Apprentissage Supervis√©
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ Variable cible continue (nombres) ?
‚îÇ   ‚îÇ   ‚îî‚îÄ OUI ‚Üí R√âGRESSION
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ Variable cible cat√©gorielle (classes) ?
‚îÇ       ‚îî‚îÄ OUI ‚Üí CLASSIFICATION
‚îÇ
‚îî‚îÄ NON ‚Üí Apprentissage Non Supervis√©
    ‚îÇ
    ‚îú‚îÄ Voulez-vous grouper des observations similaires ?
    ‚îÇ   ‚îî‚îÄ OUI ‚Üí CLUSTERING
    ‚îÇ
    ‚îú‚îÄ Voulez-vous trouver des observations anormales ?
    ‚îÇ   ‚îî‚îÄ OUI ‚Üí D√âTECTION D'ANOMALIES
    ‚îÇ
    ‚îî‚îÄ Voulez-vous r√©duire le nombre de variables ?
        ‚îî‚îÄ OUI ‚Üí R√âDUCTION DE DIMENSIONNALIT√â

Vos donn√©es ont-elles une d√©pendance temporelle ?
‚îî‚îÄ OUI ‚Üí S√âRIES TEMPORELLES
```

### Questions √† se Poser

1. **Quel est mon objectif business ?**
   - Pr√©dire une valeur ‚Üí R√©gression
   - Classer dans des cat√©gories ‚Üí Classification
   - D√©couvrir des groupes ‚Üí Clustering
   - Trouver des anomalies ‚Üí Anomaly Detection

2. **Ai-je des labels ?**
   - Oui ‚Üí Supervis√©
   - Non ‚Üí Non supervis√©
   - Partiellement ‚Üí Semi-supervis√©

3. **Quel type de variable cible ?**
   - Continue ‚Üí R√©gression
   - Cat√©gorielle ‚Üí Classification

4. **Mes donn√©es sont-elles temporelles ?**
   - Oui ‚Üí Time Series
   - Non ‚Üí Autres approches

5. **Ai-je des contraintes ?**
   - Interpr√©tabilit√© ‚Üí Mod√®les simples (LR, DT)
   - Performance ‚Üí Ensemble methods, DL
   - Temps r√©el ‚Üí Mod√®les rapides (LR, KNN)
   - Peu de donn√©es ‚Üí Mod√®les simples, feature engineering

### Tableau R√©capitulatif

| Type de T√¢che | Supervis√© ? | Variable Cible | Use Cases Principaux |
|---------------|-------------|----------------|---------------------|
| **R√©gression** | Oui | Continue | Prix, ventes, temp√©rature |
| **Classification** | Oui | Cat√©gorielle | Spam, churn, diagnostic |
| **Clustering** | Non | Aucune | Segmentation clients |
| **Anomaly Detection** | Non/Semi | Aucune/Binaire | Fraude, maintenance |
| **Time Series** | Oui/Non | Continue | Pr√©visions, pr√©dictions |

---

## Ressources Compl√©mentaires

### Documentation
- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)
- [Choosing the right estimator](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)

### Livres
- "Pattern Recognition and Machine Learning" - Christopher Bishop
- "Hands-On Machine Learning" - Aur√©lien G√©ron

---

**Navigation**
- [‚Üê Pr√©c√©dent : Fondamentaux ML](01_machine_learning_fundamentals.md)
- [Suivant : Guide des M√©triques ‚Üí](03_metrics_guide.md)
- [Retour au README principal](../README.md)
